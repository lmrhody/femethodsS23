{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/Week3_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5qXuPYmP1WM"
      },
      "source": [
        "# Jupyter Notebook for Week 3\n",
        "## NLP with Python Chapter 1, sections 1-4\n",
        "* Student Name: \n",
        "* Date: \n",
        "* Assignment Due: \n",
        "* Instructor: Lisa Rhody\n",
        "* Methods of Text Analysis, Fall 2020\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYdNHrJYP1WN"
      },
      "source": [
        "## Natural Language Processing with Python \n",
        "\n",
        "***NOTE from Lisa:*** During this course, we'll be working through significant portions of the open access book _Natural Language Processing with Python_, by Steven Bird, Ewan Klein, and Edward Loper, first published by O'Reilly. However, we'll be following along with the online version, which is regularly updated and can be found at http://nltk.org/book. You may wish to begin by reading the Preface to the book (https://www.nltk.org/book/ch00.html), which explains some of the mechanics for reading and following along with activities. You will read the definitional distinctions between **natural** and **artificial** language. \n",
        "\n",
        "While these notebooks will offer the code that you will need to work through basic exercises, I welcome, and even encourage, you taking this opportunity to also work with some of the other exercises that the book provides which are not fully included here. \n",
        "\n",
        "While the book is designed to be used while writing directly into the Python interpreter, I'm asking fo ryou to complete thes assignments in Jupyter notebooks, which allows me to combine the copy of the text with the cells where you can type and execute code. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Started\n",
        " If you are working on your own computer, you'll want to download NLTK on your local machine. There are instructions on how to do this on the NLTK site (https://www.nltk.org/install.html). You can also use the Anaconda Distribution (https://www.anaconda.com/distribution). Once you install the NLTK package, you'll need to install necessary datasets/models for future assignments to work. \n",
        "\n",
        " ## Working with NLTK\n",
        " In order for us to work with NLTK, we need to load it into Google Colab (or Jupyter or the Python interpreter). We do this by first importing NLTK and then identifying what parts of NLTK we want to work with. Once the data is added to your working environment, you can load some into the Google Colab environment. "
      ],
      "metadata": {
        "id": "HnBO2rEaRXwO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2DcBo9lP1WN"
      },
      "source": [
        "### Chapter 1: Language Processing and Python\n",
        "It is easy to get our hands on millions of words of text. What can we do with it, assuming we can write some simple programs? In this chapter we'll address the following questions:\n",
        "\n",
        "* What can we achieve by combining simple programming techniques with large quantities of text?\n",
        "* How can we automatically extract key words and phrases that sum up the style and content of a text?\n",
        "* What tools and techniques does the Python programming language provide for such work?\n",
        "* What are some of the interesting challenges of natural language processing?\n",
        "\n",
        "This chapter is divided into sections that skip between two quite different styles. In the \"computing with language\" sections we will take on some linguistically motivated programming tasks without necessarily explaining how they work. In the \"closer look at Python\" sections we will systematically review key programming concepts. We'll flag the two styles in the section titles, but later chapters will mix both styles without being so up-front about it. We hope this style of introduction gives you an authentic taste of what will come later, while covering a range of elementary concepts in linguistics and computer science. If you have basic familiarity with both areas, you can skip to 5; we will repeat any important points in later chapters, and if you miss anything you can easily consult the online reference material at http://nltk.org/. If the material is completely new to you, this chapter will raise more questions than it answers, questions that are addressed in the rest of this book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DTTpql3P1WO"
      },
      "source": [
        "### 1 Computing with Language: Texts and Words\n",
        "\n",
        "We're all very familiar with text, since we read and write it every day. Here we will treat text as raw data for the programs we write, programs that manipulate and analyze it in a variety of interesting ways. But before we can do this, we have to get started with the Python interpreter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMugD_JSP1WO"
      },
      "source": [
        "**NOTE from Lisa**: Because we are working in Jupyter notebooks and not in the **interpreter** (which is defined below), your interface will look a little bit different. That's ok. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-c027tP1WO"
      },
      "source": [
        "### 1.1   Getting Started with Python\n",
        "One of the friendly things about Python is that it allows you to type directly into the interactive **interpreter** â€” the program that will be running your Python programs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9yxgF8VP1WP"
      },
      "source": [
        "**NOTE from Lisa**: The Jupyter notebook works differently than the interpreter. Rather than having a ```>>>> ``` you will write your code directly in a cell and then run it. The result will display below the code cell. \n",
        "\n",
        "That means you can type the equation directly into the cell. When you press either the >| button above or you press the shift + enter keys, you will \"execute\" the cell, which means that it will run, and your cursor will go to the next cell. \n",
        "\n",
        "In the cell below, type the following: \n",
        "```python\n",
        "1 + 5 * 2 - 3 \n",
        "```\n",
        "\n",
        "Then press the shift + enter (or return) key. The expected action should be that the answer appears below the cell next to the word ```\n",
        "OUT []: \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFwl08uuP1WQ"
      },
      "outputs": [],
      "source": [
        "#type your equation here.\n",
        "1 + 5 * 2 - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8kCOUazP1WQ"
      },
      "source": [
        "The preceding examples demonstrate how you can work interactively with the Python interpreter, experimenting with various expressions in the language to see what they do. Now let's try a nonsensical expression to see how the interpreter handles it: Try typing 1 + (without anything after the plus sign). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "acwkaZORP1WQ"
      },
      "outputs": [],
      "source": [
        "#type 1+ below and run the cell  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV8mH6XZP1WR"
      },
      "source": [
        "This produced a syntax error. In Python, it doesn't make sense to end an instruction with a plus sign. The Python interpreter indicates the line where the problem occurred (line 1 of <stdin>, which stands for \"standard input\").\n",
        "\n",
        "Now that we can use the Python interpreter, we're ready to start working with language data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opYK1qoRP1WU"
      },
      "source": [
        "### 1.2 Getting Started with NLTK\n",
        "**NOTE from Lisa:** This portion of the chapter is about downloading and installing the text corpora that we are using first. You completed this activity during the Week 2 Jupyter notebook assignment, so I am going to skip the portion of the lesson here, because downloading and installing NLTK's corpora is time and resource intensive for your computer. So the next few cells will deviate from the book a bit. If you would like to re-do this part or if the following section does not work for you, you will want to revisit this section of NLP with Python book (https://www.nltk.org/book/ch01.html).\n",
        "\n",
        "We will pick up at the point in which we import the corpora (different from downloading, importing means that we are making those resources available to Python for use). \n",
        "\n",
        "When we get started, we need to let Python know that we are working with certain resources. These resources can be called \"libraries\" or \"packages.\" We are also able to make particular parts of a package available. We'll do this by saying that from NLTK, we want to import into existing memeory all of the texts that are in the \"book\" portion of NLTK. We do that with the following: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGFTgwisP1WU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you have run the above cell correctly, the last two lines should read \"Done Downloading collection book and then \"True.\" \n",
        "\n",
        "In this next part, you're going to use Python to tell the computer \"from NLTK's book module, load all items.\" The book module contains the data that you'll need for this chapter, which is a corpus of book text that has been pre-prepared for use with NLTK. It's important to know that these texts are already cleaned for you, as it will be much neater and easier to work with than if you were working with another plain text file that was not prepared in advance. We'll look at files that have not been prepared later, but for now, let's work with the files that come as part of NLTK. "
      ],
      "metadata": {
        "id": "twQJ5k4VS06S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "A0C3JDwxQbjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYyLM0XyP1WV"
      },
      "source": [
        "You have now told Python that you want to have NLTK ready and from NLTK's book corpora, you want to install everything (the * is used as a wild card in many computer programs). So, the correct answer should look like a list of titles. They begin text1:, text2:, etc. In other words, rather than having to type \"Moby Dick by Herman Melville 1951\" every time you want to use it, NLTK has given it a shortened name: text1. If you give Python this name, it will return to you the title of the text. Try that in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzGyJmrP1WV"
      },
      "outputs": [],
      "source": [
        "text1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCiU4B9dP1WV"
      },
      "outputs": [],
      "source": [
        "text2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tepjMc8SP1WV"
      },
      "source": [
        "### 1.3   Searching Text\n",
        "There are many ways to examine the context of a text apart from simply reading it. A concordance view shows us every occurrence of a given word, together with some context. Here we look up the word monstrous in Moby Dick by entering text1 followed by a period, then the term concordance, and then placing **\"monstrous\"** in parentheses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYVQ9iBsP1WW"
      },
      "outputs": [],
      "source": [
        "text1.concordance(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L0dSnFWP1WW"
      },
      "source": [
        "**Your Turn:** Try searching for other words; to save re-typing, you might be able to use up-arrow, Ctrl-up-arrow or Alt-p to access the previous command and modify the word being searched. You can also try searches on some of the other texts we have included. For example, search Sense and Sensibility for the word affection, using\n",
        "```  \n",
        "text2.concordance(\"affection\")\n",
        "```\n",
        "\n",
        "Search the book of Genesis to find out how long some people lived, using\n",
        "``` \n",
        "text3.concordance(\"lived\")\n",
        "```\n",
        "\n",
        "You could look at text4, the Inaugural Address Corpus, to see examples of English going back to 1789, and search for words like nation, terror, god to see how these words have been used differently over time. We've also included text5, the NPS Chat Corpus: search this for unconventional words like im, ur, lol. (Note that this corpus is uncensored!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhIZBt31P1WW"
      },
      "outputs": [],
      "source": [
        "text2.concordance(\"feminine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wyymqRlP1WW"
      },
      "source": [
        "Once you've spent a little while examining these texts, we hope you have a new sense of the richness and diversity of language. In the next chapter you will learn how to access a broader range of text, including text in languages other than English.\n",
        "\n",
        "A concordance permits us to see words in context. For example, we saw that _monstrous_ occurred in contexts such as the \\___  pictures and a \\___ size . What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLY5zPSGP1WW"
      },
      "outputs": [],
      "source": [
        "text1.similar(\"monstrous\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpjYhGuLP1WW"
      },
      "outputs": [],
      "source": [
        "text2.similar(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXQascC7P1WW"
      },
      "source": [
        "Observe that we get different results for different texts. Austen uses this word quite differently from Melville; for her, _monstrous_ has positive connotations, and sometimes functions as an intensifier like the word _very_.\n",
        "\n",
        "The term ```common_contexts ``` allows us to examine just the contexts that are shared by two or more words, such as _monstrous_ and _very_. We have to enclose these words by square brackets as well as parentheses, and separate them with a comma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhA4X1jKP1WW"
      },
      "outputs": [],
      "source": [
        "text2.common_contexts([\"monstrous\", \"very\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj-14yh8P1WX"
      },
      "source": [
        "**Your turn:** pick another pair of words and compare their usage in two different texts, using the ```similar()``` and ```common_contexts()``` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "st1AlicGP1WX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBtyJ3tP1WX"
      },
      "source": [
        "It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a **dispersion plot.** Each stripe represents an instance of a word, and each row represents the entire text. In 1.2 (https://www.nltk.org/book/ch01.html#fig-inaugural) we see some striking patterns of word usage over the last 220 years (in an artificial text constructed by joining the texts of the Inaugural Address Corpus end-to-end). You can produce this plot as shown below. You might like to try more words (e.g., _liberty_, _constitution_), and different texts. Can you predict the dispersion of a word before you view it? As before, take care to get the quotes, commas, brackets and parentheses exactly right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11JJDM0bP1WX"
      },
      "outputs": [],
      "source": [
        "text4.dispersion_plot([\"citizens\",\"democracy\", \"freedom\", \"duties\", \"America\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umjup0hbP1WX"
      },
      "source": [
        "**Important Note**: You need to have Python's NumPy and Matplotlib packages installed in order to produce the graphical plots used in this book. If you have already installed Anaconda, these should already be on your computer.\n",
        "\n",
        "**Note** You can also plot the frequency of word usage through time using https://books.google.com/ngrams. Which is to say that *this* is the function that Google's N-grams viewer is making use of. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skHPGL-kP1WX"
      },
      "source": [
        "**Also note** There is an exercise in the http://nltk.org book that uses a function to generate random text based on a book from the corpus. This feature no long works in Python 3.0 or above, but is expected to be added back in eventually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLjqer-wP1WX"
      },
      "source": [
        "### 1.4   Counting Vocabulary\n",
        "The most obvious fact about texts that emerges from the preceding examples is that they differ in the vocabulary they use. In this section we will see how to use the computer to count the words in a text in a variety of useful ways. As before, you will jump right in and experiment with the Python interpreter, even though you may not have studied Python systematically yet. Test your understanding by modifying the examples, and trying the exercises at the end of the chapter.\n",
        "\n",
        "Let's begin by finding out the length of a text from start to finish, in terms of the words and punctuation symbols that appear. We use the term len to get the length of something, which we'll apply here to the book of Genesis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYcWBD_CP1WX"
      },
      "outputs": [],
      "source": [
        "len(text3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09rOwFzjP1WX"
      },
      "source": [
        "So Genesis has 44,764 words and punctuation symbols, or \"tokens.\" A **token** is the technical name for a sequence of characters â€” such as ```hairy, his, or :) ``` â€” that we want to treat as a group. When we count the number of tokens in a text, say, the phrase _to be or not to be_, we are counting occurrences of these sequences. Thus, in our example phrase there are two occurrences of _to_, two of _be_, and one each of _or_ and _not_. But there are only four distinct vocabulary items in this phrase. How many distinct words does the book of Genesis contain? To work this out in Python, we have to pose the question slightly differently. The vocabulary of a text is just the set of tokens that it uses, since in a set, all duplicates are collapsed together. In Python we can obtain the vocabulary items of ```text3 ``` with the command: ```set(text3)```. When you do this, many screens of words will fly past. Now try the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2a8su9tP1WY"
      },
      "outputs": [],
      "source": [
        "sorted(set(text3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpokVwkkP1WY"
      },
      "outputs": [],
      "source": [
        "len(set(text3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzgaMclP1WY"
      },
      "source": [
        "By wrapping sorted() around the Python expression ```set(text3)```, we obtain a sorted list of vocabulary items, beginning with various punctuation symbols and continuing with words starting with _A_. All capitalized words precede lowercase words. We discover the size of the vocabulary indirectly, by asking for the number of items in the set, and again we can use ```len``` to obtain this number. Although it has 44,764 tokens, this book has only 2,789 distinct words, or \"word types.\" A **word type** is the form or spelling of the word independently of its specific occurrences in a text â€” that is, the word considered as a unique item of vocabulary. Our count of 2,789 items will include punctuation symbols, so we will generally call these unique items **types** instead of word types.\n",
        "\n",
        "Now, let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average (remember if you're using Python 2, to start with ```from __future__ import division```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXN1Jgl2P1WY"
      },
      "outputs": [],
      "source": [
        "len(set(text3)) / len(text3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJxgJ3txP1WY"
      },
      "source": [
        "Next, let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB-17P87P1WY"
      },
      "outputs": [],
      "source": [
        "text3.count(\"smote\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLKW9au4P1WY"
      },
      "outputs": [],
      "source": [
        "100 * text4.count('a') / len(text4) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fTAj1GeP1WY"
      },
      "source": [
        "**Exercise:** How many times does the word _lol_ appear in ```text5```? How much is this as a percentage of the total number of words in this text? [Use the first cell to count the instances of \"lol\" and the second cell to calculate the percentage of the entire text.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cdqNUYOPP1WZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fw02hXq6P1WZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQMdSAIUP1WZ"
      },
      "source": [
        "You may want to repeat such calculations on several texts, but it is tedious to keep retyping the formula. Instead, you can come up with your own name for a task, like \"lexical_diversity\" or \"percentage\", and associate it with a block of code. Now you only have to type a short name instead of one or more complete lines of Python code, and you can re-use it as often as you like. The block of code that does a task for us is called a **function**, and we define a short name for our function with the keyword ```def```. The next example shows how to define two new functions, ```lexical_diversity()``` and ```percentage()```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Lb-rg3dLP1WZ"
      },
      "outputs": [],
      "source": [
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AVs0MeURP1WZ"
      },
      "outputs": [],
      "source": [
        "def percentage(count, total):\n",
        "    return 100 * count / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkXn8_G8P1WZ"
      },
      "source": [
        "Notice that once you type the first row of the function, the cursor automatically inserts a tab. That's because the Python interpreter knows to expect an **indented code block**. If you were working in the actual Python interpreter, you would need to remember to insert that tab yourself either with four space bars or the tab key. To finish the indented block, just press enter/return. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSJmQ1wuP1WZ"
      },
      "source": [
        "In the definition of ```lexical_diversity()```, we specify a **parameter** named ```text```. This parameter is a \"_placeholder_\" for the actual text whose lexical diversity we want to compute, and reoccurs in the block of code that will run when the function is used. Similarly, ```percentage()``` is defined to take two parameters, named ```count``` and ```total```.\n",
        "\n",
        "Once Python knows that lexical_diversity() and percentage() are the names for specific blocks of code, we can go ahead and use these functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnkMkuE8P1WZ"
      },
      "outputs": [],
      "source": [
        "lexical_diversity(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aCrH_8KP1WZ"
      },
      "outputs": [],
      "source": [
        "lexical_diversity(text5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Uu92cJP1Wa"
      },
      "outputs": [],
      "source": [
        "percentage(4, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNNxO0ROP1Wa"
      },
      "outputs": [],
      "source": [
        "percentage(text4.count('a'), len(text4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m12X7kApP1Wa"
      },
      "source": [
        "To recap, we use or _call_ a function such as ```lexical_diversity()``` by typing its name, followed by an open parenthesis, the name of the text, and then a close parenthesis. These parentheses will show up often; their role is to separate the name of a task â€” such as ```lexical_diversity()``` â€” from the data that the task is to be performed on â€” such as ```text3```. The data value that we place in the parentheses when we call a function is an **argument** to the function.\n",
        "\n",
        "You have already encountered several functions in this chapter, such as ```len()```, ```set()```, and ```sorted()```. By convention, we will always add an empty pair of parentheses after a function name, as in ```len()```, just to make clear that what we are talking about is a function rather than some other kind of Python expression. Functions are an important concept in programming, and we only mention them at the outset to give newcomers a sense of the power and creativity of programming. Don't worry if you find it a bit confusing right now.\n",
        "\n",
        "Later we'll see how to use functions when tabulating data, as in 1.1 (https://www.nltk.org/book/ch01.html#tab-brown-types). Each row of the table will involve the same computation but with different data, and we'll do this repetitive work using a function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zlKbV2I5P1Wa"
      },
      "source": [
        "## 2   A Closer Look at Python: Texts as Lists of Words\n",
        "You've seen some important elements of the Python programming language. Let's take a few moments to review them systematically.\n",
        "\n",
        "### 2.1   Lists\n",
        "What is a text? At one level, it is a sequence of symbols on a page such as this one. At another level, it is a sequence of chapters, made up of a sequence of sections, where each section is a sequence of paragraphs, and so on. However, for our purposes, we will think of a text as nothing more than a sequence of words and punctuation. Here's how we represent text in Python, in this case the opening sentence of _Moby Dick_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Gm4MnkyLP1Wa"
      },
      "outputs": [],
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iogqIdp7P1Wa"
      },
      "outputs": [],
      "source": [
        "sent1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdXZQ3yJP1Wa"
      },
      "outputs": [],
      "source": [
        "len(sent1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYe9ZPmhP1Wa"
      },
      "outputs": [],
      "source": [
        "lexical_diversity(sent1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3DOdlG3P1Wa"
      },
      "source": [
        "Some more lists have been defined for you, one for the opening sentence of each of our texts, ```sent2 â€¦ sent9```. We inspect two of them here; you can see the rest for yourself using the Python interpreter (if you get an error which says that ```sent2``` is not defined, you need to first type ```from nltk.book import *```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CavotrMwP1Wa"
      },
      "outputs": [],
      "source": [
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoVRNrpnP1Wb"
      },
      "outputs": [],
      "source": [
        "sent2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVn_42bOP1Wb"
      },
      "outputs": [],
      "source": [
        "sent3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MWS7XCcP1Wb"
      },
      "source": [
        "**Your Turn:** Make up a few sentences of your own, by typing a name, equals sign, and a list of words, like this: ```ex1 = ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']```. Repeat some of the other Python operations we saw earlier in 1, e.g., ```sorted(ex1), len(set(ex1)), ex1.count('the')```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1jhwhliEP1Wb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3accNOvLP1Wb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MFcyDccP1Wb"
      },
      "source": [
        "A pleasant surprise is that we can use Python's addition operator on lists. Adding two lists creates a new list with everything from the first list, followed by everything from the second list:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL5efOyjP1Wb"
      },
      "outputs": [],
      "source": [
        "['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl1T74PYP1Wb"
      },
      "source": [
        "This special use of the addition operation is called **concatenation**; it combines the lists together into a single list. We can concatenate sentences to build up a text.\n",
        "\n",
        "We don't have to literally type the lists either; we can use short names that refer to pre-definied lists. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssDNnK6eP1Wb"
      },
      "outputs": [],
      "source": [
        "sent4 + sent1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBPIOZ3P1Wb"
      },
      "source": [
        "What if we want to add a single item to a list? This is known as **appending**. When we ```append()``` to a list, the list itself is updated as a result of the operation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUXkEveOP1Wb"
      },
      "outputs": [],
      "source": [
        "sent1.append(\"Some\")\n",
        "sent1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1DcTO3tP1Wb"
      },
      "source": [
        "## 2.2   Indexing Lists\n",
        "As we have seen, a text in Python is a list of words, represented using a combination of brackets and quotes. Just as with an ordinary page of text, we can count up the total number of words in ```text1``` with ```len(text1)```, and count the occurrences in a text of a particular word â€” say, ```'heaven'``` â€” using ```text1.count('heaven')```.\n",
        "\n",
        "With some patience, we can pick out the 1st, 173rd, or even 14,278th word in a printed text. Analogously, we can identify the elements of a Python list by their order of occurrence in the list. The number that represents this position is the item's index. We instruct Python to show us the item that occurs at an index such as 173 in a text by writing the name of the text followed by the index inside square brackets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JQAEA8HP1Wc"
      },
      "outputs": [],
      "source": [
        "text4[173]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7X3s05iP1Wc"
      },
      "source": [
        "We can do the converse;given a word, find the index of when it *first* occurs: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT_1FvpZP1Wc"
      },
      "outputs": [],
      "source": [
        "text4.index('awaken')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMP3LSwP1Wc"
      },
      "source": [
        "Indexes are a common way to access the words of a text, or, more generally, the elements of any list. Python permits us to access sublists as well, extracting manageable pieces of language from large texts, a technique known as **slicing**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq75tQ81P1Wc"
      },
      "outputs": [],
      "source": [
        "text5[16715:16735]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1TTebU0P1Wc"
      },
      "outputs": [],
      "source": [
        "text6[1600:1625]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xteRH0EP1Wc"
      },
      "source": [
        "Indexes have some subtleties, and we'll explore these with the help of an artificial sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tCZHzGQ0P1Wc"
      },
      "outputs": [],
      "source": [
        "sent = ['word1', 'word2', 'word3', 'word4', 'word5', \n",
        "        'word6', 'word7', 'word8', 'word9', 'word10']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWXYA2naP1Wc"
      },
      "outputs": [],
      "source": [
        "sent[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qxKeXMPP1Wc"
      },
      "outputs": [],
      "source": [
        "sent[9]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUthPGQNP1Wd"
      },
      "source": [
        "Notice that our indexes start from zero: sent element zero, written ```sent[0]```, is the first word, ```'word1'```, whereas sent element 9 is ```'word10'```. The reason is simple: the moment Python accesses the content of a list from the computer's memory, it is already at the first element; we have to tell it how many elements forward to go. Thus, zero steps forward leaves it at the first element.\n",
        "\n",
        "**Note** This practice of counting from zero is initially confusing, but typical of modern programming languages. You'll quickly get the hang of it if you've mastered the system of counting centuries where 19XY is a year in the 20th century, or if you live in a country where the floors of a building are numbered from 1, and so walking up _n-1_ flights of stairs takes you to level _n_.\n",
        "\n",
        "Now, if we accidentally use an index that is too large, we get an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43Iktx5TP1Wd"
      },
      "outputs": [],
      "source": [
        "sent[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvnjJ6z6P1Wd"
      },
      "source": [
        "This time it is not a syntax error, because the program fragment is syntactically correct. Instead, it is a **runtime error**, and it produces a ```Traceback``` message that shows the context of the error, followed by the name of the error, ```IndexError```, and a brief explanation.\n",
        "\n",
        "Let's take a closer look at slicing, using our artificial sentence again. Here we verify that the slice ```5:8``` includes sent elements at indexes 5, 6, and 7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkwUGInuP1Wd"
      },
      "outputs": [],
      "source": [
        "sent[5:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwEyIun-P1Wd"
      },
      "outputs": [],
      "source": [
        "sent[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plQYNXJLP1Wd"
      },
      "outputs": [],
      "source": [
        "sent[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWiYG8oJP1Wd"
      },
      "outputs": [],
      "source": [
        "sent[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGLpl2I9P1Wd"
      },
      "source": [
        "By convention, ```m:n``` means elements ```mâ€¦n-1```. As the next example shows, we can omit the first number if the slice begins at the start of the list, and we can omit the second number if the slice goes to the end:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8OpE8HUP1Wd"
      },
      "outputs": [],
      "source": [
        "sent[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIu0bJ9DP1Wd"
      },
      "outputs": [],
      "source": [
        "text2[141525:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws1_fX9cP1Wd"
      },
      "source": [
        "We can modify an element of a list by assigning to one of its index values. In the next example, we put sent[0] on the left of the equals sign. We can also replace an entire slice with new material. A consequence of this last change is that the list only has four elements, and accessing a later value generates an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kGC0F-yP1We"
      },
      "outputs": [],
      "source": [
        "sent[0] = 'First'\n",
        "sent[9] = 'Last'\n",
        "len(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAhj1QvuP1We"
      },
      "outputs": [],
      "source": [
        "sent[1:9] = ['Second', 'Third']\n",
        "sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e1znq1pP1We"
      },
      "outputs": [],
      "source": [
        "sent[9]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ZaLkGIP1We"
      },
      "source": [
        "**Your Turn:** Take a few minutes to define a sentence of your own and modify individual words and groups of words (slices) using the same methods used earlier. Check your understanding by trying the exercises on lists at the end of this chapter. (You can add cells for practice if you would like.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ilCsbvwZP1We"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOrAI_tbP1We"
      },
      "source": [
        "### 2.3   Variables\n",
        "\n",
        "From the start of 1, you have had access to texts called ```text1, text2,``` and so on. It saved a lot of typing to be able to refer to a 250,000-word book with a short name like this! In general, we can make up names for anything we care to calculate. We did this ourselves in the previous sections, e.g., defining a **variable** ```sent1```, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XneLRneMP1We"
      },
      "outputs": [],
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB-Js7STP1We"
      },
      "source": [
        "Such lines have the form: *variable = expression*. Python will evaluate the expression, and save its result to the variable. This process is called **assignment**. It does not generate any output; you have to type the variable on a line of its own to inspect its contents. The equals sign is slightly misleading, since information is moving from the right side to the left. It might help to think of it as a left-arrow. The name of the variable can be anything you like, e.g., ```my_sent, sentence, xyzzy```. It must start with a letter, and can include numbers and underscores. Here are some examples of variables and assignments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XoGs9PNrP1We"
      },
      "outputs": [],
      "source": [
        "my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',\n",
        "           'forth', 'from', 'Camelot', '.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeRNVR1NP1We"
      },
      "outputs": [],
      "source": [
        "noun_phrase = my_sent[1:4]\n",
        "noun_phrase\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgPo_3yGP1We"
      },
      "outputs": [],
      "source": [
        "wOrDs = sorted(noun_phrase)\n",
        "wOrDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OkFSy5P1Wf"
      },
      "source": [
        "Remember that capitalized words appear before lowercase words in sorted lists.\n",
        "\n",
        "**Note**: Notice in the previous example that we split the definition of my_sent over two lines. Python expressions can be split across multiple lines, so long as this happens within any kind of brackets. Python uses the \"...\" prompt to indicate that more input is expected. It doesn't matter how much indentation is used in these continuation lines, but some indentation usually makes them easier to read.\n",
        "\n",
        "It is good to choose meaningful variable names to remind you â€” and to help anyone else who reads your Python code â€” what your code is meant to do. Python does not try to make sense of the names; it blindly follows your instructions, and does not object if you do something confusing, such as ```one = 'two'``` or ```two = 3```. The only restriction is that a variable name cannot be any of Python's reserved words, such as ```def, if, not, and import```. If you use a reserved word, Python will produce a syntax error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zqXWSdlP1Wf"
      },
      "outputs": [],
      "source": [
        "not = 'Camelot'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeqPP9dDP1Wf"
      },
      "source": [
        "We will often use variables to hold intermediate steps of a computation, especially when this makes the code easier to follow. Thus ```len(set(text1))``` could also be written:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj2n2kLmP1Wf"
      },
      "outputs": [],
      "source": [
        "vocab = set(text1)\n",
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmtr8U8UP1Wf"
      },
      "source": [
        "**Caution!**\n",
        "\n",
        "Take care with your choice of names (or **identifiers**) for Python variables. First, you should start the name with a letter, optionally followed by digits (0 to 9) or letters. Thus, ```abc23``` is fine, but ```23abc``` will cause a syntax error. Names are case-sensitive, which means that ```myVar``` and ```myvar``` are distinct variables. Variable names cannot contain whitespace, but you can separate words using an underscore, e.g., ```my_var```. Be careful not to insert a hyphen instead of an underscore: ```my-var``` is wrong, since Python interprets the \"-\" as a minus sign."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJF4lbbtP1Wf"
      },
      "source": [
        "### 2.4   Strings\n",
        "Some of the methods we used to access the elements of a list also work with individual words, or strings. For example, we can assign a string to a variable (eg ```name = 'Monty'```), index a string (eg. Each letter of the word Monty becomes an index of 5 letters: M, o, n, t, y), and slice a string (eg. select only a few letters from the index of letters that make up the word Monty): [Note from Lisa: I've changed the text here for clarity...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viqnWD4SP1Wf"
      },
      "outputs": [],
      "source": [
        "name = 'Monty'\n",
        "name[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR55zupNP1Wf"
      },
      "outputs": [],
      "source": [
        "name[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfCBuv0DP1Wg"
      },
      "source": [
        "We can also perform multiplication and addition with strings: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oox5jGuSP1Wg"
      },
      "outputs": [],
      "source": [
        "name * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rbXE-rOP1Wg"
      },
      "outputs": [],
      "source": [
        "name + '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv8b6QHEP1Wg"
      },
      "source": [
        "We can join the words of a list to make a single string, or split a string into a list, as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF7tUsbFP1Wg"
      },
      "outputs": [],
      "source": [
        "' '.join(['Monty', 'Python'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05vxNzUFP1Wg"
      },
      "outputs": [],
      "source": [
        "'Monty Python'.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azFBHzLMP1Wg"
      },
      "source": [
        "We sill come back to the topic of strings in Chapter 3. For the time being, we have two important building blocks -- lists and strings -- and are ready to get back to some language analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0MbNmTvP1Wg"
      },
      "source": [
        "## 3   Computing with Language: Simple Statistics\n",
        "Let's return to our exploration of the ways we can bring our computational resources to bear on large quantities of text. We began this discussion in [1](https://www.nltk.org/book/ch01.html#sec-computing-with-language-texts-and-words), and saw how to search for words in context, how to compile the vocabulary of a text, how to generate random text in the same style, and so on.\n",
        "\n",
        "In this section we pick up the question of what makes a text distinct, and use automatic methods to find characteristic words and expressions of a text. As in [1](https://www.nltk.org/book/ch01.html#sec-computing-with-language-texts-and-words), you can try new features of the Python language by copying them into the interpreter, and you'll learn about these features systematically in the following section.\n",
        "\n",
        "Before continuing further, you might like to check your understanding of the last section by predicting the output of the following code. You can use the interpreter to check whether you got it right. If you're not sure how to do this task, it would be a good idea to review the previous section before continuing further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXOpx25aP1Wg"
      },
      "outputs": [],
      "source": [
        "saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
        "          'more', 'is', 'said', 'than', 'done']\n",
        "tokens = set(saying)\n",
        "tokens = sorted(tokens)\n",
        "tokens[-2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVy-pAe5P1Wg"
      },
      "source": [
        "### 3.1   Frequency Distributions\n",
        "How can we automatically identify the words of a text that are most informative about the topic and genre of the text? Imagine how you might go about finding the 50 most frequent words of a book. One method would be to keep a tally for each vocabulary item, like that shown in [3.1](https://www.nltk.org/book/ch01.html#fig-tally). The tally would need thousands of rows, and it would be an exceedingly laborious process â€” so laborious that we would rather assign the task to a machine.\n",
        "\n",
        "![tally.png](attachment:tally.png)\n",
        "\n",
        "The table in [3.1](https://www.nltk.org/book/ch01.html#fig-tally) is known as a **frequency distribution**, and it tells us the frequency of each vocabulary item in the text. (In general, it could count any kind of observable event.) It is a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items. Since we often need frequency distributions in language processing, NLTK provides built-in support for them. Let's use a ```FreqDist``` to find the 50 most frequent words of *Moby Dick*:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1BMb5W6P1Wh"
      },
      "outputs": [],
      "source": [
        "fdist1 = FreqDist(text1)\n",
        "print(fdist1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBxn7sCzP1Wh"
      },
      "outputs": [],
      "source": [
        "fdist1.most_common(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f13LetQUP1Wh"
      },
      "outputs": [],
      "source": [
        "fdist1['whale']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcuIOHriP1Wh"
      },
      "source": [
        "When we first invoke FreqDist, we pass the name of the text as an argument. We can inspect the total number of words (\"outcomes\") that have been counted up â€” 260,819 in the case of *Moby Dick*. The expression ```most_common(50)``` gives us a list of the 50 most frequently occurring types in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeMi6FY2P1Wh"
      },
      "source": [
        "**Your Turn:** Try the preceding frequency distribution example for yourself, for ```text2```. Be careful to use the correct parentheses and uppercase letters. If you get an error message NameError: name 'FreqDist' is not defined, you need to start your work with ```from nltk.book import * ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L5RsmpgeP1Wh"
      },
      "outputs": [],
      "source": [
        "#experiment here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu-2epVGP1Wh"
      },
      "source": [
        "Do any words produced in the last example help us grasp the topic or genre of this text? Only one word, *whale*, is slightly informative! It occurs over 900 times. The rest of the words tell us nothing about the text; they're just English \"plumbing.\" What proportion of the text is taken up with such words? We can generate a cumulative frequency plot for these words, using ```fdist1.plot(50, cumulative=True)```, to produce the graph below. These 50 words account for nearly half the book!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist1.plot(50, cumulative=True)"
      ],
      "metadata": {
        "id": "CYEpcV8MULVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the frequent words don't help us, how about the words that occur once only, the so-called **hapaxes**? View them by typing ```fdist1.hapaxes()```. This list contains _lexicographer, cetological, contraband, expostulations_, and about 9,000 others. It seems that there are too many rare words, and without seeing the context we probably can't guess what half of the hapaxes mean in any case! Since neither frequent nor infrequent words help, we need to try something else."
      ],
      "metadata": {
        "id": "S5akm3LNUJ3w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojSHBV-jP1Wh"
      },
      "outputs": [],
      "source": [
        "fdist1.hapaxes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mwk_LEkP1Wh"
      },
      "source": [
        "## 3.2   Fine-grained Selection of Words\n",
        "Next, let's look at the _long_ words of a text; perhaps these will be more characteristic and informative. For this we adapt some notation from set theory. We would like to find the words from the vocabulary of the text that are more than 15 characters long. Let's call this property _P_, so that _P(w)_ is true if and only if w is more than 15 characters long. Now we can express the words of interest using mathematical set notation as shown in [1a](https://www.nltk.org/book/ch01.html#ex-set-comprehension-math). This means \"the set of all _w_ such that _w_ is an element of _V_ (the vocabulary) and _w_ has property _P_\".\n",
        "\n",
        "The corresponding Python expression is given in (1b). (Note that it produces a list, not a set, which means that duplicates are possible.) Observe how similar the two notations are. Let's go one more step and write executable Python code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbrwsfT3P1Wi"
      },
      "outputs": [],
      "source": [
        "V = set(text9)\n",
        "long_words = [w for w in V if len(w) > 15]\n",
        "sorted(long_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAIR8JaOP1Wi"
      },
      "source": [
        "For each word ```w``` in the vocabulary ```V```, we check whether ```len(w)``` is greater than 15; all other words will be ignored. We will discuss this syntax more carefully later. \n",
        "\n",
        "**Your Turn**: Try out the previous statements in a cell of your own, and experiment with changing the text and changing the length condition. Does it make a difference to your results if you change the variable names, e.g., using ```[word for word in vocab if ...]```?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9yrzVgTP1Wi"
      },
      "outputs": [],
      "source": [
        "V = set(text7)\n",
        "long_words = [w for w in V if len(w) > 15]\n",
        "sorted(long_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gYsfC-AP1Wi"
      },
      "source": [
        "Let's return to our task of finding words that characterize a text. Notice that the long words in text4 reflect its national focus â€” _constitutionally, transcontinental_ â€” whereas those in text5 reflect its informal content: _boooooooooooglyyyyyy and yuuuuuuuuuuuummmmmmmmmmmm_. Have we succeeded in automatically extracting words that typify a text? Well, these very long words are often hapaxes (i.e., unique) and perhaps it would be better to find _frequently occurring long words_. This seems promising since it eliminates frequent short words (e.g., _the_) and infrequent long words (e.g. _antiphilosophists_). Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvYVAtfWP1Wi"
      },
      "outputs": [],
      "source": [
        "fdist5 = FreqDist(text5)\n",
        "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Ec3tyxP1Wi"
      },
      "source": [
        "Notice hwo we have used two conditions: ```len(w) > 7 ``` ensures that the words are longer than seven letters and ```fdist5[w] > 7 ```ensures that these words occur more than seven times. At last we have managed to automatically identify the frequently-occurring content-bearing words of the text. It is a modest but important miestone: a tiny piece of code, processing tens of thousands of words, produces some informative output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBT9BsJZP1Wi"
      },
      "source": [
        "### 3.3   Collocations and Bigrams\n",
        "A **collocation** is a sequence of words that occur together unusually often. Thus *red wine* is a collocation, whereas *the wine* is not. A characteristic of collocations is that they are resistant to substitution with words that have similar senses; for example, *maroon wine* sounds definitely odd.\n",
        "\n",
        "To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as *bigrams*. This is easily accomplished with the function ```nltk.bigrams()```: \n",
        "\n",
        "(**NOTE** This is a fix that is not reflected in the book. Bigrams are being handled differently in NLTK. The other way to do this is to add one line that reads: ```from nltk.util import bigrams ```)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmY6lLvPP1Wi"
      },
      "outputs": [],
      "source": [
        "list(nltk.bigrams(['more', 'is', 'said', 'than', 'done']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evdIe81fP1Wi"
      },
      "source": [
        "Here we see that the pair of words _than-done_ is a bigram, and we write it in Python as ```('than', 'done')```. Now, collocations are essentially just frequent bigrams, except that we want to pay more attention to the cases that involve rare words. In particular, we want to find bigrams that occur more often than we would expect based on the frequency of the individual words. The ```collocations()``` function does this for us. We will see how it works later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtlTPl94P1Wj"
      },
      "outputs": [],
      "source": [
        "text4.collocations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kh4bAhwP1Wj"
      },
      "outputs": [],
      "source": [
        "text8.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QqgJfRlP1Wj"
      },
      "source": [
        "Th collocations that emerge are very specific to the genre of the texts. In order to find *red wine* as a collocation, we  would need to process a much larger body of text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWx_vzKP1Wj"
      },
      "source": [
        "### 3.4   Counting Other Things\n",
        "Counting words is useful, but we can count other things too. For example, we can look at the distribution of word lengths in a text, by creating a ```FreqDist``` out of a long list of numbers, where each number is the length of the corresponding word in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUMUPr9DP1Wj"
      },
      "outputs": [],
      "source": [
        "[len(w) for w in text1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMQ5YV17P1Wj"
      },
      "outputs": [],
      "source": [
        "fdist = FreqDist(len(w) for w in text1)\n",
        "print(fdist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD7d4SCxP1Wj"
      },
      "outputs": [],
      "source": [
        "fdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "youfx7V0P1Wj"
      },
      "source": [
        "We start by deriving a list of the lengths of words in text1, and the FreqDist then counts the number of times each of these occurs. The result is a distribution containing a quarter of a million items, each of which is a number corresponding to a word token in the text. But there are at most only 20 distinct items being counted, the numbers 1 through 20, because there are only 20 different word lengths. I.e., there are words consisting of just one character, two characters, ..., twenty characters, but none with twenty one or more characters. One might wonder how frequent the different lengths of word are (e.g., how many words of length four appear in the text, are there more words of length five than length four, etc). We can do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLWNZKYAP1Wk"
      },
      "outputs": [],
      "source": [
        "fdist.most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ7tH-tKP1Wk"
      },
      "outputs": [],
      "source": [
        "fdist.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n40HG8_CP1Wk"
      },
      "outputs": [],
      "source": [
        "fdist[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YURHA8etP1Wk"
      },
      "outputs": [],
      "source": [
        "fdist.freq(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.plot(50, cumulative=False)"
      ],
      "metadata": {
        "id": "Gq9-c3TWVCct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbNQ7wqjP1Wk"
      },
      "source": [
        "From this we see that the most frequent word length is 3, and that words of length 3 account for roughly 50,000 (or 20%) of the words making up the book. Although we will not pursue it here, further analysis of word length might help us understand differences between authors, genres, or languages.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}