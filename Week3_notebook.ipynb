{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/Week3_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9uNBodd_uxs"
      },
      "source": [
        "# Jupyter Notebook for Week 3\n",
        "\n",
        "* Student Name: \n",
        "* Date: \n",
        "* Assignment Due: \n",
        "* Instructor: Lisa Rhody\n",
        "* Methods of Text Analysis, Fall 2019\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D06bG8X7_uxt"
      },
      "source": [
        "For the following assignment, you will be working through exercises from _Natural Language Processing with Python: Analyzing Text with the Natural Langauge Toolkit_ by Steven Bird, Ewan Klein, and Edward Loper. The book can be found at http://www.nltk.org/book/. For those students who are working on their own workbooks using Jupyter and GitHub, it might be helpful to have this window open on 1/2 of your computer monitor and the text of the NLP with Python book open right next to it on the other 1/2 of your screen.\n",
        "\n",
        "For those of you who are using Google Colab, this asignment includes a few extra steps in order to have things work in the Colab environment, so you will not be able to follow the book exactly. \n",
        "\n",
        "Periodically, throughout the notebook, you will be prompted to add a \"markdown\" cell and to write a reflection that connects your secondary reading with the code that you are running in the Jupyter notebook. \n",
        "\n",
        "I encourage you to add cells and to create little experiments as you go. If you do, please alert me to them by inserting a markdown cell directly above or below your experiment to explain what you are doing. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Started\n",
        " If you are working on your own computer, you'll want to download NLTK on your local machine. There are instructions on how to do this on the NLTK site (https://www.nltk.org/install.html). You can also use the Anaconda Distribution (https://www.anaconda.com/distribution). Once you install the NLTK package, you'll need to install necessary datasets/models for future assignments to work. \n",
        "\n",
        " ## Working with NLTK\n",
        " In order for us to work with NLTK, we need to load it into Google Colab (or Jupyter or the Python interpreter). We do this by first importing NLTK and then identifying what parts of NLTK we want to work with. Once the data is added to your working environment, you can load some into the Google Colab environment. "
      ],
      "metadata": {
        "id": "Wwk6WCCWFtIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ua8GkDU4_uxt"
      },
      "outputs": [],
      "source": [
        "## first import the NLTK package\n",
        "import nltk \n",
        "## then use Python to download the \"book\" section of NLTK\n",
        "nltk.download(\"book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you have run the above cell correctly, the last two lines should read \"Done Downloading collection book and then \"True.\" \n",
        "\n",
        "In this next part, you're going to use Python to tell the computer \"from NLTK's book module, load all items.\" The book module contains the data that you'll need for this chapter, which is a corpus of book text that has been pre-prepared for use with NLTK. It's important to know that these texts are already cleaned for you, as it will be much neater and easier to work with than if you were working with another plain text file that was not prepared in advance. We'll look at files that have not been prepared later, but for now, let's work with the files that come as part of NLTK. "
      ],
      "metadata": {
        "id": "ORHMGc3XHtUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We are telling Colab to download all the data inside the book package in NLTK. \n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "P5Y33hT4Dxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the success of our import by simply calling up the label that has been applied to the texts. In this case, the texts are labeled text1, text2, text3, etc. "
      ],
      "metadata": {
        "id": "I9Ic_u71IhH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "id": "5BoNgbHpFNgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2"
      ],
      "metadata": {
        "id": "XEXyMtFcFQIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By importing the NLTK book package and then the book corpora, we have access to 9 different full text books. In the next sections, we're going to search, count, and manipulate texts using NLTK. "
      ],
      "metadata": {
        "id": "5VpfmSVsIx25"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-iecc7I_uxy"
      },
      "source": [
        "# Searching Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klGLLc7K_uxy"
      },
      "outputs": [],
      "source": [
        "text1.concordance(\"monstrous\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSS2INgG_uxz"
      },
      "source": [
        "**REFLECTION:** In _The Textual Condition_, McGann writes: \"Both the practice and the study of human culture comprise a network of symbolic exchanges. Because human beings are not angels, these exchanges always involve material negotiations.\" What is the \"symbolic exchange\" that happens in creating a concordance like the one above? How are these \"material negotiations\" or not? Consider how this may or may not relate to the use of the method \"similar\" below. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gef3spsFPbrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG7YjUYy_uxz"
      },
      "outputs": [],
      "source": [
        "text1.similar(\"monstrous\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt6xK5s-_ux0"
      },
      "outputs": [],
      "source": [
        "text2.common_contexts([\"monstrous\", \"very\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNKEVu3D_ux0"
      },
      "source": [
        "**REFLECTION:** How does your sense of \"context\" and \"symbolic exchange\" change (or not) when you view the frequency of a word's use distributed across a novel using a \"dispersion plot.\"? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcbqooX7_ux0"
      },
      "outputs": [],
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McB-iAQx_ux0"
      },
      "source": [
        "# 1.4 Counting Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydUl2cO_ux1"
      },
      "source": [
        "Follow along in http://www.nltk.org/book/ch01.html by reading the description and then running the cells below. What do the authors mean by a \"token\"? How is a \"token\" different from \"text\"? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vurclQK_ux1"
      },
      "outputs": [],
      "source": [
        "len(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5xVUYDv1_ux1"
      },
      "outputs": [],
      "source": [
        "sorted(set(text3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MXfhmDT_ux1"
      },
      "outputs": [],
      "source": [
        "len(set(text3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6cDRy9K_ux2"
      },
      "source": [
        "What is the difference between \"word types\" and a \"types\"? How are these meanings distinct from other uses of the word \"types\" that you are familiar with? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NffnyGlx_ux2"
      },
      "outputs": [],
      "source": [
        "len(set(text3)) / len(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxIYa0BU_ux2"
      },
      "outputs": [],
      "source": [
        "text3.count(\"smote\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZJK3qMx_ux3"
      },
      "outputs": [],
      "source": [
        "100 * text4.count('a') / len(text4)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjQe4XWr_ux4"
      },
      "outputs": [],
      "source": [
        "def lexical_diversity(text):\n",
        "    return len(set(text)) / len(text)\n",
        "def percentage(count, total):\n",
        "    return 100 * count / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPxOLfxl_ux4"
      },
      "outputs": [],
      "source": [
        "lexical_diversity(text3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-iQkkJo_ux4"
      },
      "outputs": [],
      "source": [
        "lexical_diversity(text5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_XwAkUn_ux5"
      },
      "outputs": [],
      "source": [
        "percentage(4, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VbLmQNK_ux5"
      },
      "outputs": [],
      "source": [
        "percentage(text4.count('a'), len(text4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFrIFmIu_ux5"
      },
      "source": [
        "**REFLECTION:** McGann writes on page 4 that \"Today, texts are largely imagined as scenes of reading rather than scenes of writing. This 'readerly' view of text has been most completely elaborated through the modern hermeneutical tradition in which text is not something we _make_ but something we _interpret_.\" How does McGann's view of the text relate to the \"reading\" that happens in this notebook? Consider his later question: \"Where--and how--does the activity of reading take place?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5tFze_o_ux5"
      },
      "source": [
        "# Chapter 2 Accessing Text Corpora and Lexical Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwa9iUc_ux5"
      },
      "source": [
        "Ok, now we're going to shift a bit and look at Chapter 2 of the NLP book http://www.nltk.org/book/ch02.html. (Don't worry, we'll come back to the rest of Chapter 1 next week.) McGann talks about different _types_ of text in his introduction: \"Even the most 'informational' text comprises an interactive mechanism of communicative exchange.\" We're going to look at different texts in the NLTK corpus. Again, the easiest way to proceed here is to open Chapter 2 in a window on one side of your monitor and this notebook in the other and to read simultaneously. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_G273wc_ux6"
      },
      "outputs": [],
      "source": [
        "# remember, we already ran import nltk earlier in the notebook\n",
        "nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JMAtkSY_ux6"
      },
      "outputs": [],
      "source": [
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "len(emma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgc4ewO3_ux6"
      },
      "outputs": [],
      "source": [
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma.concordance(\"surprize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W67rxKm0_ux6"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJQDZAK-_ux8"
      },
      "outputs": [],
      "source": [
        "emma = gutenberg.words('austen-emma.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtX7el1R_ux8"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgXmipbp_ux8"
      },
      "outputs": [],
      "source": [
        "for fileid in gutenberg.fileids():\n",
        "    num_chars = len(gutenberg.raw(fileid))\n",
        "    num_words = len(gutenberg.words(fileid))\n",
        "    num_sents = len(gutenberg.sents(fileid))\n",
        "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ5_dfk4_ux9"
      },
      "outputs": [],
      "source": [
        "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOax-YCB_ux9"
      },
      "source": [
        "# 1.2 Web and Chat Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot_7Zux7_ux9"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import webtext\n",
        "for fileid in webtext.fileids():\n",
        "    print(fileid, webtext.raw(fileid)[:65], '...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmeiWa_j_ux9"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import nps_chat\n",
        "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
        "chatroom[123]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkp6Ztld_ux-"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2UGSLpe_uyS"
      },
      "source": [
        "# 1.3 Brown Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K1L-zit_uyT"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "news_text = brown.words(categories='news')\n",
        "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
        "modals = ['can','could','may','might','must','will']\n",
        "for m in modals:\n",
        "    print(m + \":\", fdist[m], end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovf2Qzxy_uyT"
      },
      "outputs": [],
      "source": [
        "brown.sents(categories=['news','editorial','reviews'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrI9nzf0_uyT"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "        (genre,word)\n",
        "        for genre in brown.categories()\n",
        "        for word in brown.words(categories=genre))\n",
        "genres = ['news', 'relgion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "cfd.tabulate(conditions=genres, samples=modals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA1KI5gR_uyT"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import inaugural\n",
        "inaugural.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNjkseDQ_uyU"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import reuters\n",
        "reuters.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR8LBPHp_uyU"
      },
      "outputs": [],
      "source": [
        "reuters.categories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx4i9tMs_uyV"
      },
      "source": [
        "You can skip the remainder of the Reuters section. We'll return to this later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhL8UxH8_uyV"
      },
      "source": [
        "# 1.5 Inaugural Address Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Jj-ZsPfc_uyV"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import inaugural\n",
        "inaugural.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0goRRZ9_uyV"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "        (target, fileid[:4])\n",
        "        for fileid in inaugural.fileids()\n",
        "        for w in inaugural.words(fileid)\n",
        "        for target in ['america', 'citizen']\n",
        "        if w.lower().startswith(target))\n",
        "cfd.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvD3a5JD_uyV"
      },
      "source": [
        "# 1.6 Annotated Text Corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5uoH2Gt_uyW"
      },
      "source": [
        "We will return to annotated text corpora later. You can choose to run the cells in this section if you wish. If you prefer to skip ahead, that is fine, too. However, consider in this section of the assignment the challenges that language differences present to text analysis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWD7Z8oH_uyW"
      },
      "outputs": [],
      "source": [
        "nltk.corpus.cess_esp.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UsIm6ns_uyW"
      },
      "outputs": [],
      "source": [
        "nltk.corpus.floresta.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKilvPe0_uyX"
      },
      "outputs": [],
      "source": [
        "nltk.corpus.indian.words('hindi.pos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDGLzhqe_uyX"
      },
      "outputs": [],
      "source": [
        "nltk.corpus.udhr.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K5bc4jf_uyX"
      },
      "outputs": [],
      "source": [
        "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqOvALff_uyX"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import udhr\n",
        "languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
        "             'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "          (lang, len(word))\n",
        "          for lang in languages\n",
        "          for word in udhr.words(lang + '-Latin1'))\n",
        "cfd.plot(cumulative=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tkcRoaL_uyY"
      },
      "outputs": [],
      "source": [
        "help(nltk.corpus.reader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXLuN13v_uyY"
      },
      "outputs": [],
      "source": [
        "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
        "raw[1:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bObpWbj1_uyY"
      },
      "outputs": [],
      "source": [
        "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
        "words[1:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo9a8sDL_uyY"
      },
      "outputs": [],
      "source": [
        "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
        "sents[1:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFeUiCK3_uyZ"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import PlaintextCorpusReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiR6ZIc-_uyZ"
      },
      "outputs": [],
      "source": [
        "corpus_root = '/usr/share/dict'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGYVMSss_uyZ"
      },
      "outputs": [],
      "source": [
        "wordlists = PlaintextCorpusReader(corpus_root, '.*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W5pyNva_uyZ"
      },
      "outputs": [],
      "source": [
        "wordlists.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE_Ejr_M_uyZ"
      },
      "outputs": [],
      "source": [
        "wordlists.words('connectives')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaVpw46M_uyZ"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "my_url = \"https://www.gutenberg.org/files/32/32-0.txt\"\n",
        "file = urlopen(my_url)\n",
        "raw = file.read()\n",
        "herland = raw.decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YKdk9B4_uya"
      },
      "outputs": [],
      "source": [
        "len(herland)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkd0d0Cy_uya"
      },
      "outputs": [],
      "source": [
        "her_tokens = nltk.word_tokenize(herland)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i_y8UQv_uya"
      },
      "outputs": [],
      "source": [
        "len(her_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuKRHOaa_uya"
      },
      "outputs": [],
      "source": [
        "herland.concordance(\"man\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "4v3F7E7j_uya"
      },
      "source": [
        "Why do you think the statement above threw an error? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVinwNwY_uyb"
      },
      "outputs": [],
      "source": [
        "print(herland)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMkAQZpg_uyb"
      },
      "source": [
        "**REFLECTION**: In _The Textual Condition_ McGann discusses the difference between \"text\" and \"paratext.\" In the printed version of Herland (above), what would be considered \"paratext\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6Up6Qm__uyb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K1VI9UQ_uyc"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "print(word_tokenize(herland))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq2K1EHY_uyc"
      },
      "outputs": [],
      "source": [
        "print(sent_tokenize(herland))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSeU_rSn_uyc"
      },
      "outputs": [],
      "source": [
        "phrases = sent_tokenize(herland)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR-iEo8O_uyc"
      },
      "outputs": [],
      "source": [
        "words = word_tokenize(herland)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0ShWRLu_uyc"
      },
      "outputs": [],
      "source": [
        "print(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lHSpJxL_uyd"
      },
      "outputs": [],
      "source": [
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te2SuOp7_uyd"
      },
      "source": [
        "**Reflection**: How does the definition of _text_ employed by McGann differ from the definition of _text_ employed by the Natural Language Processing book? What similar assumptions might be made about what a text is? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKK8Oj4Z_uyd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}