{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/Week10notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 10 Notebook\n",
        "\n",
        "Name: \n",
        "\n",
        "Date:\n",
        "\n",
        "Class: \n",
        "\n",
        "Notes: (Anything you'd like to share with me before I read your notebook)"
      ],
      "metadata": {
        "id": "gUfYy6SOb9rl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KkeKuUTcSdz"
      },
      "source": [
        "This week's notebook is based on Chapter 5 from \n",
        "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text) by\n",
        "Jens Albrecht, Sidharth Ramachandran, Christian Winkler. I've added cells to this week's notebook to help explain \n",
        "\n",
        "# Vectors, Features, and Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review: Preparing Data for Operationalization\n",
        "- Tokenizing: breaking documents into units (words, characters, sentences) called tokens. \n",
        " - Multiple tokens can be grouped together into n-grams. \n",
        "- Stemming: \n",
        " - Words = root + prefix, suffix\n",
        " - Root = where the core meaning is held\n",
        " - Stemming reduced words that have similar meanings but multiple forms, such as tense, plural, gerunds, etc. \n",
        "- Dimension Reduction\n",
        "  - Simplify the number of observations \n",
        "  - Stopword reduction (removing words that don't convey \"meaninig\") \n",
        "  - Remove words that are too frequent or too unique\n",
        "  - remove other terms that are likely to confuse the counting. \n"
      ],
      "metadata": {
        "id": "Hb0Tnb1BFkQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectors\n",
        "Vectors are mathematical objects that encode length and direction (represents a position or change in a framework or space) A 1-dimensional array of numbers (components) is displayed as a distribution. When represented geometrically, vectors represent coordinates in an n-dimensional space where n is the number of dimensions (units being compared). \n",
        "- In machine learning, text is represented in an array of numbers\n",
        "- Natural extension of real numbers in mathematics is a tuple (pairs of two numbers) \n",
        "- Vectors are useful because they are numerical representations that are spatial and therefore have norms and distances\n",
        "- We use spatial properties to measure similarity\n",
        "- Measuring similarity is a fundamental principle for analyzing texts with computers\n",
        "- Occupying the same, or related space, represents similarities between vectors\n"
      ],
      "metadata": {
        "id": "i5LPUIbcG6Ph"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmiG4-d4cSd2"
      },
      "source": [
        "# Feature Engineering and Syntactic Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tf0t1VEcSd2"
      },
      "source": [
        "## Things to consider\n",
        "\n",
        "While this notebook follows Chapter 5 from *Blueprints for Text Analysis with Python*, I have also made some changes. Some changes reflect updates and modifications to arguments or methods in the code, while other changes are about trying to streamline and focus on the concept of vectorizing rather than looking too closely at computing power and resource management, which is something that you have to begin to take into consideration. For example, in the second half of the assignment, we'll be working with the [ABC News Headline dataset from Kaggle](https://www.kaggle.com/datasets/therohk/million-headlines) Rather than going into detail about how to complete the lemmatizing in batches, I truncate the dataset so that we can improve computing time. \n",
        "\n",
        "If preparing text corpora for machine learning is something that you are interested in doing, you'll want to spend some time learning about batch processing and maybe even working with a remote high power computing environment to improve the performance of the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxzIVYZcSd5"
      },
      "source": [
        "## Setup<div class='tocSkip'/>\n",
        "\n",
        "We are going to use the set up and directory locations from the book. If working on Google Colab: copy files and install required libraries. The following cell imports a github repository for chapter 5. The files are downloaded into the /content directory. During this phase, you're also installing the package Spacy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqnnhnkecSd6"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "ON_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if ON_COLAB:\n",
        "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
        "    os.system(f'wget {GIT_ROOT}/ch05/setup.py')\n",
        "\n",
        "%run -i setup.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ez2JUfcSd7"
      },
      "source": [
        "## Load Python Settings<div class=\"tocSkip\"/>\n",
        "\n",
        "Common imports, defaults for formatting in Matplotlib, Pandas etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znlis3YjcSd7"
      },
      "outputs": [],
      "source": [
        "%run \"$BASE_DIR/settings.py\"\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = 'png'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpBXXtbkcSd8"
      },
      "source": [
        "# Data preparation\n",
        "We begin by creating a very small corpus of sentences that come from the opening lines of *A Tale of Two Cities* by Charles Dickens. We're going to work on a very simplified version of vectorization. Our hope is that by working with this simplified example that it becomes easier to then extend that mental model to much larger tasks. \n",
        "\n",
        "The following cell takes the list of sentences and then tokenizes the words in each sentence. Next, we are going through each token in the list `sentences` and we are taking each word and putting it into a list that we store as `vocabulary`. Finally, we import Pandas and we use the `enumerate` function to create a list of tuples that include each word as a string and the position of the word in the list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBck02vTcSd8"
      },
      "outputs": [],
      "source": [
        "sentences = [\"It was the best of times\", \n",
        "             \"it was the worst of times\", \n",
        "             \"it was the age of wisdom\", \n",
        "             \"it was the age of foolishness\"]\n",
        "\n",
        "tokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]\n",
        "\n",
        "vocabulary = list(dict.fromkeys([w for s in tokenized_sentences for w in s]))\n",
        "\n",
        "import pandas as pd\n",
        "[[w, i] for i,w in enumerate(vocabulary)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0bKs-hGcSd8"
      },
      "source": [
        "# One-hot by hand\n",
        "In the following cell, we're defining a function called onehot_encode. That function goes through the list of `tokenized_sentences` one word at a time and checks it against the vocabulary. If a word from the vocabulary exists in the tokenized sentence, then it places a 1 in a list. If the word from the vocabulary does not occur in the sentence, it places a 0. \n",
        "\n",
        "The result is a vector of binaries: ones and zeroes that represent the sentence by whether or not words in the vocabulary are present. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scEWPZq_cSd9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def onehot_encode(tokenized_sentence):\n",
        "    return [1 if w in tokenized_sentence else 0 for w in vocabulary]\n",
        "\n",
        "onehot = [onehot_encode(tokenized_sentence) for tokenized_sentence in tokenized_sentences]\n",
        "\n",
        "for (sentence, oh) in zip(sentences, onehot):\n",
        "    print(\"%s: %s\" % (oh, sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a sample vector created with a sentence that wasn't\n",
        "# included in the vocabulary list but has some of the words from the vocabulary.\n",
        "\n",
        "onehot_encode(\"the age of wisdom is the best of times\".split())"
      ],
      "metadata": {
        "id": "A3dz6PTiB7XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If none of the words in a sentence appear in the vocabulary list, then\n",
        "# you will end up with a vector that has all null values. \n",
        "onehot_encode(\"John likes to watch movies. Mary likes to watch movies too.\".split())"
      ],
      "metadata": {
        "id": "MKwhi4HgCD45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Document-Term Matrix\n",
        "The document term matrix is the vector representation of all documents adn is the most basic building block for nearly all machine learning tasks we will do this semester. Each sentence from the list of sentences is represented in a row. Each of the words in the vocabulary is represented in the columns. The following cell takes the one_hot encoding and turns it into a dataframe. "
      ],
      "metadata": {
        "id": "1bsdI5-4CQKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(onehot, columns=vocabulary)"
      ],
      "metadata": {
        "id": "trtMKPDCBR2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating similarities\n",
        "Calculating the similarities between documents works by calculating the number of common 1s at the corresponding positions. In one-hot encoding, this is an extremely fast operation, as it can be calculated on the bit level by ANDing the vectors and counting the number of 1s in the resulting vectors. "
      ],
      "metadata": {
        "id": "WV-dNAMT0u2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btvNjODNcSd9"
      },
      "outputs": [],
      "source": [
        "# calculate the similarities between the first 2 sentences\n",
        "# the result is the number of 1s that are shared between the 2 sentences. \n",
        "sim = [onehot[0][i] & onehot[1][i] for i in range(0, len(vocabulary))]\n",
        "sum(sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scalar Product or Dot Product\n",
        "calculated by multiplying corresponding components of the two vectors and adding up the products. If a product can only be 1 if both factors are 1, we can calculate the number of common 1s in the vectors. "
      ],
      "metadata": {
        "id": "hiGtORmwDPwn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWo_4mkLcSd9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.dot(onehot[0], onehot[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbSkMKCgcSd-"
      },
      "source": [
        "## Out of vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdOOgKCMcSd-"
      },
      "outputs": [],
      "source": [
        "onehot_encode(\"the age of wisdom is the best of times\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EixfjMIbcSd-"
      },
      "outputs": [],
      "source": [
        "onehot_encode(\"John likes to watch movies. Mary likes movies too.\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK7XTAyxcSd-"
      },
      "source": [
        "## document term matrix\n",
        "A matrix is a vector of vectors. In other words, each of the sentences is represented by a vector of 0s and 1s that represent whether or not a word in the vocabulary appears in the sentence. When you take all four vectors and put them in a list, it becomes a matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XcX-mCbcSd-"
      },
      "outputs": [],
      "source": [
        "onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtnfMq33cSd-"
      },
      "source": [
        "## Similarities\n",
        "\n",
        "We can also create a matrix of the similarity scores. So, in the following matrix example, we calculate the scalar or dot product similarity of each sentence compared to each of the other sentences in the corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqiS-KFScSd-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.dot(onehot, np.transpose(onehot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX3VrQ4EcSd_"
      },
      "source": [
        "## Scikit learn one-hot vectorization\n",
        "\n",
        "We did onehot vectorization by hand, but you can also use a tool like scikit learn to do it. Scikit Learn's OneHotEncoder is designed for the specific purpose of categorizing features (it encodes the features into the data), and that's not what we're doing here. We just want to see the encoding in action, so we use the MultiLabelBinarizer (because we want to make it as complicated as possible...). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T01OoDOmcSd_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "lb = MultiLabelBinarizer()\n",
        "lb.fit([vocabulary])\n",
        "lb.transform(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: \n",
        "Vectorizing has two different methods: fit and transform. Fit *learns* the vocabulary. Transform *converts* the documents into vectors. "
      ],
      "metadata": {
        "id": "LZvbaevMmDrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Statistics\n",
        "Counting words is the simplest approach to analyzing language and text. While language is fluid and dynamic, there are some predictable elements. For example, we can count on the fact that there are words that are frequently found across all documents and all language domains (even poetry). The most common words used in English documents are: the, of, to, and, in, is, for, The, that, and said. Inversely, rare words or words that appear only one time in a corpus are also very common and can comprise about 1/2 of the total words. These are *hapax legomena* (words that appear only once). \n",
        "\n",
        "Two \"laws\" describe this phenomeonon in text analysis: [*Zipf's law*](https://en.wikipedia.org/wiki/Zipf%27s_law) and [*Heaps' Law*](https://en.wikipedia.org/wiki/Heaps%27_law#:~:text=Heaps'%20law%20means%20that%20as,the%20distinct%20terms%20are%20drawn). \n"
      ],
      "metadata": {
        "id": "7lJ7Y1VzeiUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words Models\n",
        "Bag-of-words representations create vectors for documents that also preserve the frequency of words that appear in each document as a feature. The frequency of the words are used as part of the weighting of the model. Models like Latent Dirichlet Allocation (LDA) explicitly require a BoW approach. \n"
      ],
      "metadata": {
        "id": "Ajsfdl08mg8S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tphaAYI7cSd_"
      },
      "source": [
        "# CountVectorizer\n",
        "Creating vectorizers from scratch can be very time intensive, and since a similar method can be repurposed for several types of modeling, we can use the same algorithm over and over again. If we use scikit-learn, that algorithm is part of the class called CountVectorizer. The process of turning documents into vectors is also called feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxloZUJQcSd_"
      },
      "outputs": [],
      "source": [
        "#import sklearn's CountVectorizer and then rename the function as cv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_x-P5pVcSd_"
      },
      "outputs": [],
      "source": [
        "more_sentences = sentences + [\"John likes to watch movies. Mary likes movies too.\",\n",
        "                              \"Mary also likes to watch football games.\"]\n",
        "pd.DataFrame(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKHNFs2WcSd_"
      },
      "outputs": [],
      "source": [
        "# Use the CountVectorizer to learn the new vocabulary in more_sentences\n",
        "cv.fit(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGMnOjRucSd_"
      },
      "outputs": [],
      "source": [
        "# then print out the whole vocabulary\n",
        "print(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlYeJuFkcSd_"
      },
      "outputs": [],
      "source": [
        "# then we use transform to vectorize all the sentences in the more_sentences variable \n",
        "dt = cv.transform(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRBXyJLwcSeA"
      },
      "outputs": [],
      "source": [
        "# when we call the vairable dt, it will describe the matrix of vectors that CountVectorizer produced\n",
        "# that matrix is a vector of vectors\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKtDVAlwcSeA"
      },
      "outputs": [],
      "source": [
        "# When we turn the vector of vectors (or matrix) into a dataframe, \n",
        "# we can see the features for each sentence \n",
        "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Cosign Similarities\n",
        "If you want to find similarities between documents, the trick is harder than just finding the 1s. The number of occurrences of each word can be greater, and that needs to be given additional weight in our calculation of similarity. We can use the angle between two vectors to measure the similarity between them. The output is limited to numbers between 0 and 1 with 0 representing no similarity and 1 representing exact similarity.  Scikit-learn has a function that helps us to do this. "
      ],
      "metadata": {
        "id": "AAYbEzKnqbvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXKxWJ39cSeA"
      },
      "outputs": [],
      "source": [
        "#import cosine_similarity and then use it to calculate the angle between the first and second sentences.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(dt[0], dt[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is8-XdIwcSeA"
      },
      "outputs": [],
      "source": [
        "# the more_sentences variable has 6 sentences (or docs), so we will need to compare each of the\n",
        "# six sentences to each of the other sentences. \n",
        "len(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6yBWpbHcSeA"
      },
      "outputs": [],
      "source": [
        "# Each sentence is a row and a column. The numbers they share are their calculated\n",
        "# similarity. Obviously, document 1 and document 1 are the same, so their cosine \n",
        "# similarity score is 1. The added sentences have no overlap with the first 4, so \n",
        "# their cosine similarity is 0. \n",
        "pd.DataFrame(cosine_similarity(dt, dt))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review\n",
        "Vectorizing textual data turns strings of vocabulary into a numerical array, which also becomes a set of features that we can use to compare one document to another. One way to do this is with the Bag-of-Words approach. Bag-of-words vectorizing does not take the order of the vocabulary into account, but it does take the frequency that a word appears in a document and gives that term higher weight. \n",
        "\n",
        "In the next section, we'll work with TF-IDF vectorizing which essentially \"punishes\" words that appear too frequently in the corpus. "
      ],
      "metadata": {
        "id": "wr_lLojctv6G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860VEIStcSeA"
      },
      "source": [
        "# TF/IDF\n",
        "\n",
        "Term Frequency - Inverse Document Frequency (tf-idf) counts the number of total word occurrences in the corpus in addition to the occurrences in a single document. It uses the relationship between the frequency that a term is used in a document and compares it to the frequency that it appears in the entire collection so that words that are frequently found in both one document and in all documents is then inversely weighted overall. \n",
        "\n",
        "The logic behind this approach is that if there are words that are used frequently in every document, then they are not likely to convey important information. Instead, important information will likely be found in uncommon words that convey something different. \n",
        "\n",
        "Inverted document frequency creates a penalty for common words. In fact, we can arrive at a tf-idf weighting even if we start with a Bag-of-Words matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQU0w5--cSeA"
      },
      "outputs": [],
      "source": [
        "# from the feature_extraction text package in sklearn, we import TfidfTransformer\n",
        "# then we fit and transform the variable dt from above. \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf_dt = tfidf.fit_transform(dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKUk4fArcSeA"
      },
      "outputs": [],
      "source": [
        "# next, we turn the matrix into a dataframe so we can see it. \n",
        "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5-i2NEVcSeA"
      },
      "outputs": [],
      "source": [
        "# We can do the same cosine similarity calculation on the tf-idf matrix\n",
        "pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DmFRn84cSeB"
      },
      "outputs": [],
      "source": [
        "# Let's look at a more \"real world\" example. The ABC News headline\n",
        "# dataset are headlines from the Australian news outlet from 2013-2017.\n",
        "# We read in the files as a csv, convert it to a dataframe and print the first few entries. \n",
        "headlines = pd.read_csv(ABCNEWS_FILE, parse_dates=[\"publish_date\"])\n",
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how many rows the dataset has. \n",
        "len(headlines.index)"
      ],
      "metadata": {
        "id": "J6OMkTca3k8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgFPPrl0cSeB"
      },
      "outputs": [],
      "source": [
        "# That's a lot of rows! We can't possibly do this by hand. \n",
        "# The following tkaes the sklearn tfidf vectorizer, learns the vocabulary, then \n",
        "# builds the vectors for each headline. \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW1yyRECcSeB"
      },
      "outputs": [],
      "source": [
        "# If we look at a description of the new dataframe, we see that the size of the \n",
        "# dataframe becomes enormous and most of the positions in the dataframe are \n",
        "# null values, which makes it very sparse.\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4n3YGjxcSeB"
      },
      "outputs": [],
      "source": [
        "dt.data.nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5xJ2vtOcSeB"
      },
      "outputs": [],
      "source": [
        "# One of the biggest challenges we face because of the size of the dataframe\n",
        "# is the amount of time that it would take to calculate similarity scores. \n",
        "# The following just calculates it for the first 10,000 rows. \n",
        "%%time\n",
        "cosine_similarity(dt[0:10000], dt[0:10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ispAR1cSeB"
      },
      "source": [
        "# Dimension Reduction\n",
        "In order to make the operation / model run more efficiently, we need to reduce the number of dimensions (vector spaces) in the dataset. We can do this by removing stopwords using a set vocabulary list, removing the most frequent and infrequently used words, and combining words by reducing them just to their word roots so that the vocabulary is smaller. That's what the next few cells will do. \n",
        "\n",
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3PAlE-IcSeB"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "print(len(stopwords))\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dpHDkfcSeB"
      },
      "source": [
        "## min_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ1rYnigcSeB"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bynyGJK-cSeC"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", min_df=.0001)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCIjlcFhcSeC"
      },
      "source": [
        "## max_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RzR7DLmcSeC"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_df=0.1)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pf3w0jncSeC"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_df=0.1)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUz7ForecSeC"
      },
      "source": [
        "## n-grams\n",
        "You are not limited to just looking at the similarity between single words. You can calculate the similarities between 1, 2, and 3 words that appear together. That will create a bit more contextual information. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-7mt_sLcSeC"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "print(dt.shape)\n",
        "print(dt.data.nbytes)\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,3), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "print(dt.shape)\n",
        "print(dt.data.nbytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06Dr9K69cSeC"
      },
      "source": [
        "## Lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ldTRyOZcSeC"
      },
      "outputs": [],
      "source": [
        "# The following example cuts the dataset down considerably. We're only running the \n",
        "# lemmatizing on the first 25 headlines. It took about 3 hours to run the entire set\n",
        "# through the lemmatizer. Some of the values that follow will be incomplete, but it \n",
        "# should still give you a sense of how it all works. \n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nouns_adjectives_verbs = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\"]\n",
        "\n",
        "# for i, row in tqdm(headlines.iterrows(), total=len(headlines)):\n",
        "for i, row in tqdm(headlines[:24].iterrows(), total=len(headlines)):\n",
        "    doc = nlp(str(row[\"headline_text\"]))\n",
        "    headlines.at[i, \"lemmas\"] = \" \".join([token.lemma_ for token in doc])\n",
        "    headlines.at[i, \"nav\"] = \" \".join([token.lemma_ for token in doc if token.pos_ in nouns_adjectives_verbs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MvZEW98cSeC"
      },
      "outputs": [],
      "source": [
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVuKZkdVcSeC"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGh5mjJIcSeD"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3pDzR3JcSeD"
      },
      "source": [
        "## Remove top 10,000\n",
        "We could also use other dictionaries to remove common words in English. In the following example, we use a list from Google that is very popular of the most common words. Using a much larger list like this one will decrease the size of the dataframe to a more manageable size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_kyPTSicSeD"
      },
      "outputs": [],
      "source": [
        "top_10000 = pd.read_csv(\"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt\", header=None)\n",
        "tfidf = TfidfVectorizer(stop_words=list(set(top_10000.iloc[:,0].values)))\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBWVPRDYcSeD"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=list(set(top_10000.iloc[:,0].values)), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pABdRtXmcSeD"
      },
      "source": [
        "## Finding document most similar to made-up document\n",
        "\n",
        "Using this process, we can do something similar to what we did with the one hot encoding above. We can take a sentence that is not included in the vocabulary, vectorize it, and then calculate how similar it is to sentences in the existing corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGgBFlwfcSeD"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVGBKTiecSeD"
      },
      "outputs": [],
      "source": [
        "made_up = tfidf.transform([\"australia and new zealand discuss optimal apple size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpsDgrLucSeD"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(made_up, dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-EMCtPucSeD"
      },
      "outputs": [],
      "source": [
        "sim[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKM56zoBcSeE"
      },
      "outputs": [],
      "source": [
        "headlines.iloc[np.argsort(sim[0])[::-1][0:5]][[\"publish_date\", \"lemmas\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGafTjcecSeF"
      },
      "source": [
        "# Finding most related words\n",
        "\n",
        "We can go through the headlines now and estimate which words in the headlines are most similar to other words in headlines. In this case, we're using cosine similarity distributions to compare words that are most likely to be found in similar contexts as other words. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxkynwfgcSeF"
      },
      "outputs": [],
      "source": [
        "tfidf_word = TfidfVectorizer(stop_words=\"english\", min_df=1000)\n",
        "dt_word = tfidf_word.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FSgnSxscSeF"
      },
      "outputs": [],
      "source": [
        "r = cosine_similarity(dt_word.T, dt_word.T)\n",
        "np.fill_diagonal(r, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4ZDLygScSeG"
      },
      "outputs": [],
      "source": [
        "voc = tfidf_word.get_feature_names_out()\n",
        "size = r.shape[0] # quadratic\n",
        "for index in np.argsort(r.flatten())[::-1][0:40]:\n",
        "    a = int(index/size)\n",
        "    b = index%size\n",
        "    if a > b:  # avoid repetitions\n",
        "        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}