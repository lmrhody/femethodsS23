{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/Week8_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAkti4G4I1dy"
      },
      "source": [
        "# Week 8: Working with Text Data from APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbVLjq4pI1d0"
      },
      "source": [
        "Fill out the cell below with your information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdnDlnxFI1d0"
      },
      "source": [
        "* Student Name: \n",
        "* Date: \n",
        "* Instructor: Lisa Rhody\n",
        "* Assignment due: \n",
        "* Methods of Text Analysis\n",
        "* MA in DH at The Graduate Center, CUNY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncgW8rT_I1d0"
      },
      "source": [
        "## Objectives\n",
        "This week you'll be working with the availablility of APIs and the data they provide, as well as learing about some of the steps that you need to take while cleaning and preparing text data for analysis. \n",
        "\n",
        "In this notebook, you will:\n",
        "* ingest data into a notebook from an API; \n",
        "* use Pandas dataframes to find and organize data; \n",
        "* uncover some of the challenges of working with data, including variation, multiple words with similar word stems, words with similar meanings, stopword control, and more; \n",
        "* Consider the relationship between the data you are working with, the forms of \"data cleaning\" or \"data scrubbing\" that most text analysis piplines use, and the challenges that those methods present a feminist critical approach. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "As always, we need to begin by importing Python libraries that we know we will need to use. By this point, perhaps you are already familiar with some of them and know why we use them. Others may be less familiar to you. See if you can read through the list of imports and identify what each package does and why we will need it. "
      ],
      "metadata": {
        "id": "egrAMMlOMIBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import pprint"
      ],
      "metadata": {
        "id": "yC6MGOIjJo9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOMkI2ZhI1d9"
      },
      "source": [
        "## Access data using an API\n",
        "In the following exercise, you will import data from the Chronicling America API. You will set parameters for what content and keywords to pull in, then you will send the request to the server. After you import the data, you'll organize and clean up the JSON format--in other words, when you get your search results, it will come packaged in a file format, called JSON. We will ingest the JSON file, turn it into a dictionary, and then turn part of that dictionary into a Pandas Dataframe. All we're doing when we turn text data into a dataframe is organizing the metadata and the files into a format that can be used and acted upon in order to do other kinds of analysis. \n",
        "\n",
        "To work with APIs, we will need to import a new library called \"[requests](https://pypi.org/project/requests/).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0GeV5aCI1d9"
      },
      "outputs": [],
      "source": [
        "# Make the Requests module available\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are APIs? \n",
        "APIs are a set of routines that allow you to build from and interact with a software application. APIs make it possible for 2 software programs to work with each other. We will use APIs to pull data from applications. Many applications, like Twitter, Instagram, LinkedIn, and other applications have APIs. \n",
        "\n",
        "In the following example, you are going to use the requests library to make an http request to the [Open Movie Database](https://www.omdbapi.com/) (OMDb). We are going to use a security protocol called an API Key and request that the API return results to a query about the movie *The Princess Bride*. \n",
        "\n",
        "First, go to the API Key generator on the [OMDb website](https://www.omdbapi.com/apikey.aspx?__EVENTTARGET=freeAcct&__EVENTARGUMENT=&__LASTFOCUS=&__VIEWSTATE=%2FwEPDwUKLTIwNDY4MTIzNQ9kFgYCAQ9kFgICBw8WAh4HVmlzaWJsZWhkAgIPFgIfAGhkAgMPFgIfAGhkGAEFHl9fQ29udHJvbHNSZXF1aXJlUG9zdEJhY2tLZXlfXxYDBQtwYXRyZW9uQWNjdAUIZnJlZUFjY3QFCGZyZWVBY2N0oCxKYG7xaZwy2ktIrVmWGdWzxj%2FDhHQaAqqFYTiRTDE%3D&__VIEWSTATEGENERATOR=5E550F58&__EVENTVALIDATION=%2FwEdAAU%2BO86JjTqdg0yhuGR2tBukmSzhXfnlWWVdWIamVouVTzfZJuQDpLVS6HZFWq5fYpioiDjxFjSdCQfbG0SWduXFd8BcWGH1ot0k0SO7CfuulHLL4j%2B3qCcW3ReXhfb4KKsSs3zlQ%2B48KY6Qzm7wzZbR&at=freeAcct&Email=). Click the radio button next to FREE. Enter your email address, first, and last name, and then in the \"use\" section, you can write: \"Completing an assignment for class.\" Then click Submit. It usually takes just a few moments for a confirmation email to arrive in your email box. Be sure to click on the second link in the email first to validate your key. The email will include a sequence of characters and numbers that you will use in this exercise. Once you have set up a key, you can begin the rest of the activity. \n",
        "\n",
        "In order to retrieve data from the OMDb API using your API Key, we need to make a request using a communication protocol that is common on the internet: http. Essentially, what we will do is create a variable called `url` in which we will store an http request that is sent to the internet address www.omdbapi.com/. What follows the address, beginning with a question mark, is a query string. Query strings are not part of the URL syntax, but it tells the API (in this case) what your key is,and then what information you would like to retrieve. In this case, the information is all the information included in the record with the title *The Princess Bride*. \n",
        "\n",
        "Notice that the URL cannot contain spaces. Therefore, we insert the percent sign `%` where a space in the title might go. Alternately, we could put a + sign between each word. "
      ],
      "metadata": {
        "id": "Gqz2XQ5NvmMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.omdbapi.com/?apikey=4603ce48&t=the%princess%bride'"
      ],
      "metadata": {
        "id": "Np8o-USEwFj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a variable movies and use the get method in requests to read in the \n",
        "# response from the URL.\n",
        "movies = requests.get(url)\n",
        "# What datatype is the variable movies?\n",
        "type(movies)"
      ],
      "metadata": {
        "id": "c9vnrqww3G3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of the `requests.get()` method is specific to the requests library. In order to use the file, though, we need to convert the data from its current format into a JSON file. We do that by taking movies and applying the `.json' function."
      ],
      "metadata": {
        "id": "5U-CoW7W3yjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take movies and turn it into json. \n",
        "json_data = movies.json()\n",
        "type(json_data)"
      ],
      "metadata": {
        "id": "wnXRQHQD3qUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you check the data type now, you will discover that the json file is saved as a dictionary, which is to say a series of \"keys\" and \"values\" saved in pairs. \n",
        "\n",
        "Finally, we need to create a for loop so that we can go through the API results and print out the keys and their associated values. So, for every key and it's associated value in the json_data object, we look at each item in the dictionary and print the key, then a : and then the associated values. "
      ],
      "metadata": {
        "id": "5ouHxqvV4iB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in json_data.items():\n",
        "  print(key + ':', value)"
      ],
      "metadata": {
        "id": "165Kh2la4gCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that we can search by title in the API because of the documentation on the [OMDb website](https://www.omdbapi.com/). Look under Usage and Parameters. In fact, the OMDb site includes examples, so that you can do a search and find the query string you need to get the result you are looking for. All the search parameters are listed here. \n",
        "\n",
        "Another way to search for a particular movie is with its item ID in the IMDB database. You can find the item identifier by looking at the end of the URL when you search for a movie. For example, in this URL https://www.imdb.com/title/tt2906216/ we would use the item ID `tt2906216`. There are also additional arguments that can be used in the query string for OMDb to find the full plot description. "
      ],
      "metadata": {
        "id": "qliewevs5UQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.omdbapi.com/?apikey=4603ce48&i=tt2906216&plot=full'\n",
        "dandd = requests.get(url)\n",
        "json_data = dandd.json()\n",
        "for key, value in json_data.items():\n",
        "  print(key + ':', value)"
      ],
      "metadata": {
        "id": "Dd-M2WyF5QNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chronicling America\n",
        "For this activity, we are going to use the [Chronicling America API](https://chroniclingamerica.loc.gov/about/api/), which is created and maintained by the Library of Congress. Chronicling America is an archive of digitized newspapers from across the United States that are not under copyright protection by another digitization vendor. It is part of the National Digitial Newspaper project funded by the National Endowment for the Humanities. There are more than [140,000 newspaper titles](https://chroniclingamerica.loc.gov/search/titles/) included in the collection. \n",
        "\n",
        "If you're interested in some of the critiques of the Chronicling America project, you might want to read Benjamin Fagen's article \"Chronicling White America.\" (Fagan, Benjamin. \"Chronicling White America.\" American Periodicals: A Journal of History & Criticism, vol. 26 no. 1, 2016, p. 10-13. Project MUSE https://muse.jhu.edu/article/613375.) "
      ],
      "metadata": {
        "id": "Oe84bMNwbXRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTyHRrBcI1d9"
      },
      "outputs": [],
      "source": [
        "# Create a variable called 'api_search_url' and give it a value\n",
        "api_search_url = 'https://chroniclingamerica.loc.gov/search/pages/results/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQsPU7JnI1d9"
      },
      "outputs": [],
      "source": [
        "# This creates a dictionary called 'params' and sets values for the API's mandatory parameters\n",
        "# The parameters are drawn from the API documentation which describes the fields in the API. \n",
        "params = {\n",
        "    'proxtext': 'poetry' # Search for this keyword -- feel free to change!\n",
        "    \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH9hFU3qI1d9"
      },
      "source": [
        "(Later on, you will be asked to return to the above cell and change the search parameters. You do this by replacing `poetry` with `yourterm`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHzRgVgLI1d9"
      },
      "outputs": [],
      "source": [
        "# The following line adds a value for 'encoding' to our dictionary\n",
        "params['format'] = 'json'\n",
        "\n",
        "# Let's view the updated dictionary\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtQtczZBI1d9"
      },
      "outputs": [],
      "source": [
        "# The next line uses the requests package that we imported above to pull data from the Chronicling America API \n",
        "# and stores the result in a variable called 'response'\n",
        "response = requests.get(api_search_url, params=params)\n",
        "\n",
        "# We use a print statement to show us the url that we are sending to the API\n",
        "print('Here\\'s the formatted url that gets sent to the ChronAmerca API:\\n{}\\n'.format(response.url)) \n",
        "\n",
        "# It's nice to have some feedback about the status of the API response \n",
        "# to make sure there were no errors. The following checks to see that there are no errors and shows a response.\n",
        "if response.status_code == requests.codes.ok:\n",
        "    print('All ok')\n",
        "elif response.status_code == 403:\n",
        "    print('There was an authentication error. Did you paste your API above?')\n",
        "else:\n",
        "    print('There was a problem. Error code: {}'.format(response.status_code))\n",
        "    print('Try running this cell again.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yrvvwuHI1d-"
      },
      "outputs": [],
      "source": [
        "# We are going to take the Chronicling America API's JSON results and turn them into a Python variable called 'data'\n",
        "data = response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The request that we made was for data formatted in JSON, which means Javascript Object Notation. JSON is a structured way of organizing information and it can be converted into a Python Dataframe; however, it's not always easy for a human to read. We're going to use another package called Prettify to use indentation and color to help make the JSON a little more understandable to the human reader. We're also using a json library and a library called Pygments to add some colour to the output. "
      ],
      "metadata": {
        "id": "Qn667lxtfHLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KgaMjeSJI1d-"
      },
      "outputs": [],
      "source": [
        "# Let's prettify the raw JSON data and then display it.\n",
        "\n",
        "# We're using the Pygments library to add some colour to the output, so we need to import it\n",
        "import json\n",
        "from pygments import highlight, lexers, formatters\n",
        "import pprint\n",
        "\n",
        "# This uses Python's JSON module to output the results as nicely indented text\n",
        "formatted_data = json.dumps(data, indent=2)\n",
        "\n",
        "# This colours the text\n",
        "highlighted_data = highlight(formatted_data, lexers.JsonLexer(), formatters.TerminalFormatter())\n",
        "\n",
        "# And now display the results\n",
        "print(highlighted_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(json_data)"
      ],
      "metadata": {
        "id": "eamcN_jPotmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading text in a dataframe\n",
        "Next, you will use what we learned about the data in the API using the keys. We're going to look into the \"items\" entry in the JSON file and create a dataframe using Pandas that pulls out the title, content, and year of publication for each of the items. \n"
      ],
      "metadata": {
        "id": "S0CdHJzSnsXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drill down into the API data to find the filds we want to pull out and work with\n",
        "json_data = json.loads(formatted_data)\n",
        "# print(json_data['items'])\n",
        "cleaned_papers = []\n",
        "for item in json_data['items']:\n",
        "  # print(item['ocr_eng'])\n",
        "  cleaned_papers.append({'title': item['title'], 'content': item['ocr_eng'], 'date': item['date'] })\n",
        "# print(cleaned_papers)\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "pp.pprint(cleaned_papers)\n"
      ],
      "metadata": {
        "id": "nQJJd7IAHKKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0frc3IayI1d-"
      },
      "source": [
        "The output of the above cell will be quite long. Before turning in this assignment, please delete the cell above so the file you turn in is not difficult to read. Thank you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUsS7enaI1d-"
      },
      "outputs": [],
      "source": [
        "type(cleaned_papers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCdbRWkI1d-"
      },
      "source": [
        "In the cell below, we will take the nested dictionary, which is also a json format, and we will convert it into a DataFrame. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD_1IlQI1d-"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame.from_dict(json_data)\n",
        "print(df.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1snB5GIsI1d_"
      },
      "source": [
        "If we switch the layout of the dataframe, it becomes easier to see how the labels for the dataframe are different from the many items in the items observation. We can try to use the json method `normalize` to flatten out the file into columns. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmgkbynYI1d_"
      },
      "outputs": [],
      "source": [
        "df = pd.io.json.json_normalize(json_data)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkmSFECRI1d_"
      },
      "outputs": [],
      "source": [
        "dfpapers = pd.DataFrame.from_dict(json_data['items'])\n",
        "dfpapers.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7AxOmrSI1d_"
      },
      "outputs": [],
      "source": [
        "for key in dfpapers:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCl43lyVI1eA"
      },
      "source": [
        "The `.tail()` method will print out just the last (in this case) 6 items in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dates = dfpapers[\"date\"]\n",
        "dates.head()"
      ],
      "metadata": {
        "id": "tib4WgwqReC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfpapers[\"date\"] = pd.to_datetime(dfpapers[\"date\"])\n",
        "dfpapers[\"date\"]"
      ],
      "metadata": {
        "id": "bFvEUtjA6Jve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL2alAxxI1eA"
      },
      "source": [
        "The `shape()` method will show how many rows and how many columns are in your dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK8JCnOJI1eA"
      },
      "outputs": [],
      "source": [
        "dfpapers.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfpapers.describe()"
      ],
      "metadata": {
        "id": "1w0SZAV_vVEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTqWqrSvI1eB"
      },
      "source": [
        "## Reflection and Writing\n",
        "In this exercise, you queried an API from Chronicling America and pulled in files that included the search term \"poetry.\" Those files, then, reshaped and made slightly more tidy by highlighting the \"keys\" to the dictionary, and then taking one small section of the dictionary and turning it into a dataframe. \n",
        "\n",
        "Look back over the notebook and do the following: \n",
        "\n",
        "\n",
        "1.   Return to the section \"Reading text in a dataframe.\" Read through some of the entries. Create a new text cell and explain what kind of \"data cleaning\" you would recommend to prepare the text for analysis. How can you take into consideration Munoz and Rawson's article? What makes the data \"messy\"? What messiness should remain? What messiness should be repaired? What messiness should be removed? \n",
        "2.    Go to the top of the Chronicling America section. Make a copy of the search query and try replacing the term \"poetry\" (the parameter of the search argument) with another one. What were the results? \n",
        "3.   What changes when you rerun the activity besides the results? Do you need to make any changes to the next cells for them to run? \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JPUP3e6HqHX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}