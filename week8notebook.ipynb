{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/week8notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV79zi8ETtuj"
      },
      "source": [
        "# Week 8 Jupyter Notebook Assignment\n",
        "## Programming Historian: Introduction to stylometry with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo2lEVPoTtul"
      },
      "source": [
        "* Student Name: \n",
        "* Date: \n",
        "* Instructor: Lisa Rhody\n",
        "* Assignment due: \n",
        "* Methods of Text Analysis\n",
        "* MA in DH / MS in Data Analysis and Visualization at The Graduate Center, CUNY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEa4V2AiTtul"
      },
      "source": [
        "## Preparing for this week - Conceptualization\n",
        "This week's assignment is going to veer more toward the technical than other weeks. We are going to read about a type of sylometric analysis called authorship attribution, and we're going to work through one of the most common authorship attribution tests in DH. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9TQemSrTtul"
      },
      "source": [
        "### The following notebook is much indebted to and grateful for Programming Historian, and in particular, François Dominic Laramée. \n",
        "François Dominic Laramée, \"Introduction to stylometry with Python,\" The Programming Historian 7 (2018), https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgIRGAy6Ttum"
      },
      "source": [
        "### The assignment\n",
        "This week, you will be working through one of the most common authorship attribution activities: Assessing the likely authorship of the disputed essays in _The Federalist Papers_. As you do so, consider the readings that you have done and how they come into conversation with the methods in this activity. The assignment will work through three different approaches to the same question using 3 different assumptions about language and measuring. You may work together to run the cells of the notebooks, but individual assignments should be completed individually. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw61sMVoTtum"
      },
      "source": [
        "Your comments should reflect your consideration of what is happening functionally and theoretically throughout each experiment. Consider the following questions to help you get started: \n",
        "* Why did the author pick this particular dataset? \n",
        "* What about the selection of the dataset makes sense? What does the selection of data tell us about the kind of questions the authors expect users to have about their data? \n",
        "* There are three different approaches to estimating the likely authorship of one of the disputed essays. What are the assumptions that underlie each of the approaches to authorship attribution? \n",
        "* What are three different ways that \"authorship\" as a concept is measured? \n",
        "* Are there gendered inflections to these approaches? How so or why not? (Be sure to point to a specific cell. Give evidence from a secondary source to support your point of view.)\n",
        "* How does the result of each experiment \"answer\" the question at hand? What appeal does the article make toward the authority or correctness of the answer? \n",
        "-- Another way to ask this question is: What are the features that seem to have the strongest correlation to authorship? \n",
        "* Given our discussion of what a \"text\" is, what might these tests of authorship be missing? \n",
        "* What are the advantages and/or disadvantages to using the *Federalist Papers* as the dataset we use to study ownership of language and authorship? What is assumed? What is ignored? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfxkCtiKTtum"
      },
      "source": [
        "### Directions: \n",
        "* To complete this lesson, go to the Programming Historian Lesson: [Introduction to sylometry with Python by François Dominic Laramée](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#calculating-features-and-z-scores-for-our-test-case) using your web browser. Keep this notebook page open on one side of your monitor and follow along with the Programming Historian lesson on the other side of your screen/monitor. \n",
        "* Download the zip file in the section titled [The Dataset](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python#the-dataset). \n",
        "* Note: This Jupyter Notebook should be saved in the same folder as your Dataset folder.  \n",
        "* Add new cells above and below the cells you are annotating and type your annotations in mark down.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsERDZ7nTtum"
      },
      "source": [
        "## Historical Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SIGKA0FTtum"
      },
      "source": [
        "As Patrick Juola writes: \"Authorship Attribution is about ownership of words.\" What is at stake when we embark upon a process of trying to assign ownership to words? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9HAF8IPTtun"
      },
      "source": [
        "## Data Import and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q_N7Gav_Ttun"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baW8XJcYTtuo"
      },
      "outputs": [],
      "source": [
        "papers = {\n",
        "    'Madison': [10, 14, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],\n",
        "    'Hamilton': [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21, 22, 23, 24, \n",
        "                 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 60,\n",
        "                 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, \n",
        "                 78, 79, 80, 81, 82, 83, 84, 85],\n",
        "    'Jay': [2, 3, 4, 5],\n",
        "    'Shared': [18, 19, 20],\n",
        "    'Disputed': [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63],\n",
        "    'TestCase': [64]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download [this zip file](https://programminghistorian.org/assets/introduction-to-stylometry-with-python/stylometry-federalist.zip) onto your local computer, open it, and upload all of the files in the data folder to Google Colab. You will need to open the files menu on the left and put all the uploaded files into a new directory called \"data.\" "
      ],
      "metadata": {
        "id": "W3DqN2LJUV_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "8GQUY3kmUThz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mge8kpu4Ttuo"
      },
      "outputs": [],
      "source": [
        "# A function that compiles all of the text files associated with a single author into a single string\n",
        "def read_files_into_string(filenames):\n",
        "    strings = []\n",
        "    for filename in filenames:\n",
        "        with open(f'data/federalist_{filename}.txt') as f:\n",
        "            strings.append(f.read())\n",
        "    return '\\n'.join(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4YP4iu8Ttuo"
      },
      "outputs": [],
      "source": [
        "# Make a dictionary out of the authors' corpora\n",
        "federalist_by_author = {}  \n",
        "for author, files in papers.items():\n",
        "    federalist_by_author[author] = read_files_into_string(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "XixhrYhbTtuo"
      },
      "outputs": [],
      "source": [
        "for author in papers: \n",
        "    print(federalist_by_author[author][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjnGjAZLTtuo"
      },
      "source": [
        "## First Stylometric Test: Mendenhall's Characteristic Curves of Composition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQMZEkb7Ttuo"
      },
      "source": [
        "What is being measured? How? What assumptions are translated into measurable functions? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KWCXj6W3Ttup"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Compare the disputed papers to those written by everyone, \n",
        "# including the shared ones. \n",
        "authors = (\"Hamilton\", \"Madison\", \"Disputed\", \"Jay\", \"Shared\")\n",
        "\n",
        "# Transform the authors' corpora into lists of word tokens\n",
        "federalist_by_author_tokens = {}\n",
        "federalist_by_author_length_distributions = {}\n",
        "for author in authors:\n",
        "    tokens = nltk.word_tokenize(federalist_by_author[author])\n",
        "    \n",
        "    # Filter out punctuation\n",
        "    federalist_by_author_tokens[author] = ([token for token in tokens \n",
        "                                            if any(c.isalpha() for c in token)])\n",
        "   \n",
        "    # Get a distribution of token lengths\n",
        "    token_lengths = [len(token) for token in federalist_by_author_tokens[author]]\n",
        "    federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
        "    federalist_by_author_length_distributions[author].plot(15,title=author)     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L8n6-K7Ttup"
      },
      "source": [
        "## Second Stylometric Test: Kilgariff's Chi-Squared Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0zQzW6HTtup"
      },
      "source": [
        "What is a probability distribution? What does \"distance\" mean in the way that it is used here? Pay particular attention to the bulleted explanation of how the statistic is applied. In the following section, the code is all written together. See if you can comment out what is happening at each stage and why it matters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry83_HwbTtup"
      },
      "outputs": [],
      "source": [
        "# Who are the authors we are analyzing?\n",
        "authors = (\"Hamilton\", \"Madison\")\n",
        "\n",
        "# Lowercase the tokens so that the same word, capitalized or not,\n",
        "# counts as one word\n",
        "for author in authors:\n",
        "    federalist_by_author_tokens[author] = (\n",
        "        [token.lower() for token in federalist_by_author_tokens[author]])\n",
        "federalist_by_author_tokens[\"Disputed\"] = (\n",
        "    [token.lower() for token in federalist_by_author_tokens[\"Disputed\"]])\n",
        "\n",
        "# Calculate chisquared for each of the two candidate authors\n",
        "for author in authors:\n",
        "   \n",
        "    # First, build a joint corpus and identify the 500 most frequent words in it\n",
        "    joint_corpus = (federalist_by_author_tokens[author] + \n",
        "                    federalist_by_author_tokens[\"Disputed\"])\n",
        "    joint_freq_dist = nltk.FreqDist(joint_corpus)\n",
        "    most_common = list(joint_freq_dist.most_common(500))\n",
        "\n",
        "    # What proportion of the joint corpus is made up \n",
        "    # of the candidate author's tokens?\n",
        "    author_share = (len(federalist_by_author_tokens[author]) \n",
        "                    / len(joint_corpus))\n",
        "    \n",
        "    # Now, let's look at the 500 most common words in the candidate \n",
        "    # author's corpus and compare the number of times they can be observed \n",
        "    # to what would be expected if the author's papers \n",
        "    # and the Disputed papers were both random samples from the same distribution.\n",
        "    chisquared = 0\n",
        "    for word,joint_count in most_common:\n",
        "        \n",
        "        # How often do we really see this common word?\n",
        "        author_count = federalist_by_author_tokens[author].count(word)\n",
        "        disputed_count = federalist_by_author_tokens[\"Disputed\"].count(word)\n",
        "        \n",
        "        # How often should we see it?\n",
        "        expected_author_count = joint_count * author_share\n",
        "        expected_disputed_count = joint_count * (1-author_share)\n",
        "        \n",
        "        # Add the word's contribution to the chi-squared statistic\n",
        "        chisquared += ((author_count-expected_author_count) * \n",
        "                       (author_count-expected_author_count) / \n",
        "                       expected_author_count)\n",
        "                    \n",
        "        chisquared += ((disputed_count-expected_disputed_count) *\n",
        "                       (disputed_count-expected_disputed_count) \n",
        "                       / expected_disputed_count)\n",
        "        \n",
        "    print(\"The Chi-squared statistic for candidate\", author, \"is\", chisquared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRsNkmV9Ttup"
      },
      "source": [
        "## Third Stylometric Test: John Burrows' Delta Method (Advanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQWTKOwFTtup"
      },
      "outputs": [],
      "source": [
        "# Who are we dealing with this time?\n",
        "authors = (\"Hamilton\", \"Madison\", \"Jay\", \"Disputed\", \"Shared\")\n",
        "\n",
        "# Combine every paper except our test case into a single corpus\n",
        "whole_corpus = []\n",
        "for author in authors:\n",
        "    whole_corpus += federalist_by_author_tokens[author]\n",
        "    \n",
        "# Get a frequency distribution\n",
        "whole_corpus_freq_dist = list(nltk.FreqDist(whole_corpus).most_common(30))\n",
        "whole_corpus_freq_dist[ :10 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWo-YwKzTtuq"
      },
      "outputs": [],
      "source": [
        "# The main data structure\n",
        "features = [word for word,freq in whole_corpus_freq_dist]\n",
        "feature_freqs = {}\n",
        "\n",
        "for author in authors:\n",
        "    # A dictionary for each candidate's features\n",
        "    feature_freqs[author] = {} \n",
        "    \n",
        "    # A helper value containing the number of tokens in the author's subcorpus\n",
        "    overall = len(federalist_by_author_tokens[author])\n",
        "    \n",
        "    # Calculate each feature's presence in the subcorpus\n",
        "    for feature in features:\n",
        "        presence = federalist_by_author_tokens[author].count(feature)\n",
        "        feature_freqs[author][feature] = presence / overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz-uyQSdTtuq"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# The data structure into which we will be storing the \"corpus standard\" statistics\n",
        "corpus_features = {}\n",
        "\n",
        "# For each feature...\n",
        "for feature in features:\n",
        "    # Create a sub-dictionary that will contain the feature's mean \n",
        "    # and standard deviation\n",
        "    corpus_features[feature] = {}\n",
        "    \n",
        "    # Calculate the mean of the frequencies expressed in the subcorpora\n",
        "    feature_average = 0\n",
        "    for author in authors:\n",
        "        feature_average += feature_freqs[author][feature]\n",
        "    feature_average /= len(authors)\n",
        "    corpus_features[feature][\"Mean\"] = feature_average\n",
        "    \n",
        "    # Calculate the standard deviation using the basic formula for a sample\n",
        "    feature_stdev = 0\n",
        "    for author in authors:\n",
        "        diff = feature_freqs[author][feature] - corpus_features[feature][\"Mean\"]\n",
        "        feature_stdev += diff*diff\n",
        "    feature_stdev /= (len(authors) - 1)\n",
        "    feature_stdev = math.sqrt(feature_stdev)\n",
        "    corpus_features[feature][\"StdDev\"] = feature_stdev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2yU23jtTtuq"
      },
      "outputs": [],
      "source": [
        "feature_zscores = {}\n",
        "for author in authors:\n",
        "    feature_zscores[author] = {}\n",
        "    for feature in features:\n",
        "        \n",
        "        # Z-score definition = (value - mean) / stddev\n",
        "        # We use intermediate variables to make the code easier to read\n",
        "        feature_val = feature_freqs[author][feature]\n",
        "        feature_mean = corpus_features[feature][\"Mean\"]\n",
        "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
        "        feature_zscores[author][feature] = ((feature_val-feature_mean) / \n",
        "                                            feature_stdev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpjn7UCbTtuq"
      },
      "outputs": [],
      "source": [
        "# Tokenize the test case\n",
        "testcase_tokens = nltk.word_tokenize(federalist_by_author[\"TestCase\"])\n",
        "    \n",
        "# Filter out punctuation and lowercase the tokens\n",
        "testcase_tokens = [token.lower() for token in testcase_tokens \n",
        "                   if any(c.isalpha() for c in token)]\n",
        " \n",
        "# Calculate the test case's features\n",
        "overall = len(testcase_tokens)\n",
        "testcase_freqs = {}\n",
        "for feature in features:\n",
        "    presence = testcase_tokens.count(feature)\n",
        "    testcase_freqs[feature] = presence / overall\n",
        "    \n",
        "# Calculate the test case's feature z-scores\n",
        "testcase_zscores = {}\n",
        "for feature in features:\n",
        "    feature_val = testcase_freqs[feature]\n",
        "    feature_mean = corpus_features[feature][\"Mean\"]\n",
        "    feature_stdev = corpus_features[feature][\"StdDev\"]\n",
        "    testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev\n",
        "    print(\"Test case z-score for feature\", feature, \"is\", testcase_zscores[feature])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxDCPIqUTtuq"
      },
      "outputs": [],
      "source": [
        "for author in authors:\n",
        "    delta = 0\n",
        "    for feature in features:\n",
        "        delta += math.fabs((testcase_zscores[feature] - \n",
        "                            feature_zscores[author][feature]))\n",
        "    delta /= len(features)\n",
        "    print( \"Delta score for candidate\", author, \"is\", delta )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC9iLyBxTtuq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkkY7QR8Ttuq"
      },
      "source": [
        "## Conclusions and Further Reading and Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o3Y5Xo3Ttur"
      },
      "source": [
        "Consider one of the \"Interesting case studies\" at the end of the lesson. What are the opportunities / stakes that authorship attribution raises in each case? Are there cases when authorship attribution may not make sense to do? Are there ethical implications? How might/could authorship attribution participate in cultural or archival recovery projects? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8p8JJIGCTtur"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}