{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/Week11_notebook_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA9OTdrH0Un5"
      },
      "source": [
        "# Week 11 Notebook\n",
        "## Supervised Text Classification Algorithms\n",
        "\n",
        "Name:\n",
        "\n",
        "Date:\n",
        "\n",
        "Class:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7rJqq-3zsRk"
      },
      "source": [
        "This notebook assignment draws heavily from Chapter 6 of \n",
        "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
        "Jens Albrecht, Sidharth Ramachandran, Christian Winkler. We will be looking at supervised binary and multiclass text classifications. \n",
        "\n",
        "\n",
        "# Chapter 6:<div class='tocSkip'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-SXU3IozsRn"
      },
      "source": [
        "# How to use classification algorithms to label text into multiple categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCDO5rmczsRn"
      },
      "source": [
        "## Note from the authors of Blueprints<div class='tocSkip'/>\n",
        "\n",
        "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
        "\n",
        "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
        "\n",
        "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
        "\n",
        "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxcKKOOlzsRo"
      },
      "source": [
        "## Setup<div class='tocSkip'/>\n",
        "\n",
        "Set directory locations. If working on Google Colab: copy files and install required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZld_MbQzsRo"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "ON_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if ON_COLAB:\n",
        "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
        "    os.system(f'wget {GIT_ROOT}/ch06/setup.py')\n",
        "\n",
        "%run -i setup.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leeNBe0DzsRp"
      },
      "source": [
        "## Load Python Settings<div class=\"tocSkip\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eI8BlmZzsRp"
      },
      "outputs": [],
      "source": [
        "%run \"$BASE_DIR/settings.py\"\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%config InlineBackend.figure_format = 'png'\n",
        "\n",
        "# to print output of all statements and not just the last\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
        "pd.set_option('display.html.use_mathjax', False)\n",
        "\n",
        "# path to import blueprints packages\n",
        "sys.path.append(BASE_DIR + '/packages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-QO194wzsRq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import html \n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "## Depracated:\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "## New version:\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from blueprints.preparation import clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_c0RQePzsRq"
      },
      "source": [
        "## What you'll learn and what we will build\n",
        "This notebook will work through the three fundamental stages of doing supervised machine learning and classification analysis with text. Supervised learning means that we have a dataset with accurate labels to work from, and that we train the model using about 80% of the dataset as \"training\" data. Then, we test the accuracy of the model on a remaining subset of data (typically about 20% or less). We can validate the model if we have labels for the testing dataset. We're going to learn how to take a dataset and divide it into training and testing sets, to validate the model by evaluating its accuracy, and then applying cross-validation techniques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D8IVtZ8zsRr"
      },
      "source": [
        "# Introducing the Java Development Tools Bug Dataset\n",
        "The dataset that we're going to use is a set of bug reports about Java Development Tools. The first classification study will be to see if we can differentiate high priority bug reports from lower priority bug reports. Then, we'll try to classify different types of bug reports. Finally, we'll try to identify what features the bug reports are referring to in order to direct it to the appropriate person for fixing. \n",
        "\n",
        "We'll need to import the dataset in the cell below. Then we'll read it in as a csv file and convert it to a Pandas dataframe. To help us understand the way that the data is organized, we'll print out the labels (column headings), and then pull out of the dataframe two random entries. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nojZy27TzsRr"
      },
      "outputs": [],
      "source": [
        "file = \"eclipse_jdt.csv\"\n",
        "file = f\"{BASE_DIR}/data/jdt-bugs-dataset/eclipse_jdt.csv.gz\" ### real location\n",
        "df = pd.read_csv(file)\n",
        "print (df.columns)\n",
        "df[['Issue_id','Priority','Component','Title','Description']].sample(2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS5JoM59wkMm"
      },
      "source": [
        "## Sample data \n",
        "We're going to remove bug reports from the dataset that have been identified as Duplicate issues. In other words, the issue was already discovered and described elsewhere. If we were trying to do summary statistics about the total number of bugs or the rate of response, etc. then we might want to remove all the entries that are reported as duplicates. However, our task is about using the text to predict the priority of a bug--not the speed of response. Since *most* issues are not duplicate issues, what we'll find is that the Duplicate Issue will most often have a null value. So, we can remove that attribute of the data without impacting the reliability of our analysis. \n",
        "\n",
        "Take some time to read through a sample from the data. This will help you to better understand what some of the potential problems might be before you start modeling. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37zn9ztOzsRr"
      },
      "outputs": [],
      "source": [
        "# we want to thin the dataset out by removing duplicates.  \n",
        "\n",
        "### df = df.drop(columns=['Duplicated_issue']) - old code ###\n",
        "df = df.drop(columns=['Duplicated_issue'], axis=1)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "df.sample(1, random_state=123).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h13u4xGNqag3"
      },
      "outputs": [],
      "source": [
        "## Let's see how big the dataset is. Shape tells us the number of rows and columns\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCmsbi4zqrwB"
      },
      "outputs": [],
      "source": [
        "# Let's do some summary statistics of the dataset as we're getting started\n",
        "# so that we have a better understanding of how many of each category there is.\n",
        "df['Priority'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5yBCyMYzsRs"
      },
      "outputs": [],
      "source": [
        "# It's not always easy to understand how much of a difference there is\n",
        "# in terms of the amount of data that we're working with, so sometimes\n",
        "# creating a histogram is helpful for getting a better sense of the proportions\n",
        "df['Priority'].value_counts().sort_index().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0OtDaS-zsRs"
      },
      "outputs": [],
      "source": [
        "# Later on, we'll be looking at the prediction of which component is\n",
        "# involved in the bug report, so let's do some summary statistics of that.\n",
        "df['Component'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReL0nRtYuroZ"
      },
      "outputs": [],
      "source": [
        "# While you can see from the summary statistics that APT and Doc are relatively\n",
        "# fewer in number than other issues, one of the first things we notice is that \n",
        "# User Interface (UI) issues are most commonly the ones reported, followed by \n",
        "# bugs in the core functionality. \n",
        "df['Component'].value_counts().sort_index().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwWnjMV7tRRr"
      },
      "outputs": [],
      "source": [
        "# This activity does not predict the status of the bug based on the text\n",
        "# It's good practice to just try to understand the data that you're working with\n",
        "# But it might be helpful to think about WHY it might not be worth the time trying\n",
        "# to predict status based on the text. Or would it be worth it?\n",
        "\n",
        "df['Status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyvuAPvZtVlt"
      },
      "outputs": [],
      "source": [
        "# Again, we don't test for the resolution classification of a bug. \n",
        "# But as you go through the supervised learning activities, would this be a\n",
        "# category worth testing? Why or why not?\n",
        "df['Resolution'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xxY4wTjz_M5"
      },
      "source": [
        "## Question: \n",
        "These are not the only questions you might want to ask of yourself or of the dataset before you get started. What are some of the other questions to ask? Based on what you know already from vectorizing and doing cosine similarities, what kinds of phenomeonon in the data might impact the outcome? What are some of the things that you would recommend we do when preparing the data to a.) improve the accuracy of the model and b.) reduce the computing resources needed to do it? \n",
        "\n",
        "Consider, for example, that ecological impacts of large language models are a feminist concern, especially as changes in climate are proving most devastating to women and to countries in the Global South where the impact of climate change is felt most immediately. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwxVYWO7zsRs"
      },
      "source": [
        "# Blueprint: Building a Text Classification system\n",
        "\n",
        "Ok, now we know a little bit about the dataset that we're working with. We know how many dimensions it has. We know the basic proportions of several features, including Priority, Component, Status, and Resolution. \n",
        "\n",
        "In the next section of the notebook, we're going to work deliberately through three steps in supervised text classification. This is a blueprint or workflow that can be reproduced (with adaptations based on the dataset you're working with) for a variety of other similar kinds of text classification analyses. \n",
        "\n",
        "After we go through building a text classification system in three steps, we'll go back over the process from start to finish as a coherent workflow. \n",
        "\n",
        "## Supervised Learning\n",
        "Text classification is one form of machine learning (ML) that falls under supervised learning. As we learned from reading Meredith Broussard, supervised learning requires that we have a dataset with labeled data points. We use that data and through computational processes that compare features of each observation, the machine develops a prediction as to which features from the data are most likely related to the labels associated with it. The output of the training set is a model. In the model we are about to create, we will use independent text variables, such as title and description to predict the priority or the component associated with it. \n",
        "\n",
        "A supervised machine learning method maps a function from the input to the output based on calculating statistical likelihoods that there is a relationship between variables. That, ultimately, is what we're defining as \"learning.\" \n",
        "\n",
        "Of course, we have to know whether or not the model is reflective of what we would be satisfied to call \"reality\" or \"true.\" Therefore, we divide the process in two components. First, we \"train\" the model, which means we use the largest amount of data and labels so that the computer comes up with predictions of the relationships between the text and the label we assigned to it. Next, we use a smaller subset of the same data to test how accurate the model's predictions are. The more features we add to the model, presumably, the more likely we are to get accurate results. In other words, if we know more than Priority, but also the day of the week that the bug report is submitted or the time of day... by increasing the density of the feature vector (remember from our vectorizing assignments), the idea is that we improve the liklihood that the model \"trained\" will be accurate (which is to say reflected of our expected reality). \n",
        "\n",
        "## How it works\n",
        "During the training phase, we take text documents and their associated labels and we create vectors that both describe the text and include the presence or absence of the labeled features. The computer runs those vectors through statistical processes that calculate the likelihood that relationships between vectors exist. The output is a model that includes various statistical predictions. \n",
        "\n",
        "The next step is that we take new text data and we feed the text vectors--this time without the labels--to the trained model. Then we use the model to output the statistical likelihood that that the text is associated with each of the labels based on what it \"learned\" during the first phase. \n",
        "\n",
        "We can then predict the accuracy with which the model is able to predict the relationship between a text document and a label by comparing the predictions to the actual labels of the tested dataset. \n",
        "\n",
        "## Types of Supervised Text Classification\n",
        "\n",
        "- *Binary Classification* means that based on multiple features text is marked as either having the feature or not having it. For example, whether an email is spam or not spam. (But it can't tell you if that email is one more of those newsletters your friends send out regularly that you simply don't have time to read but that your friend is going to ask you about when you see them next time.... sorry... that one's on you.) \n",
        "\n",
        "- *Multiclass Classification* is when each observation is associated with only one label out of several options. \n",
        "\n",
        "- *Multilabeled Classification* is when each observation ould be assocaited with multiple labels. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEiCDJZCzsRs"
      },
      "source": [
        "## Step 1 - Data Preparation\n",
        "First, we're going to reduce the number of columns in the statset to just the title, description, and priority. If any of the observations in each of those entries has no data, then we want to remove that entry from the dataset. Then, we want to combine the title and the description text into one lump. That means that the dataframe turns into just 2 columns that are \"priority\" and \"text.\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U14AYa3zsRs"
      },
      "outputs": [],
      "source": [
        "df = df[['Title','Description','Priority']]\n",
        "df = df.dropna()\n",
        "df['text'] = df['Title'] + ' ' + df['Description']\n",
        "df = df.drop(columns=['Title','Description'])\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np8lrJbg7s04"
      },
      "source": [
        "We've gone over steps in cleaning data in previous weeks, so we're going to import some of those cleaning processes here to streamline things. Then we're going to only keep the data with a string longer than 50 characters. That's easy to do because there's enough data in the dataset without the smaller documents and the more text there is in the issue the better chance we have of correlating it to an accurate label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D6CNrx6zsRt"
      },
      "outputs": [],
      "source": [
        "from blueprints.preparation import clean\n",
        "df['text'] = df['text'].apply(clean)\n",
        "df = df[df['text'].str.len() > 50]\n",
        "df.sample(2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfkt6kUbzsRt"
      },
      "source": [
        "## Step 2 - Train-Test Split\n",
        "Fortunately, there are packages for machine learning that will help you to divide a dataset into two groups: a larger group to train the model and a smaller set to test it. \n",
        "\n",
        "In the following step, we're creating multiple variables. In other words, we need to split into 80% training data and 20% testing data. We also need to specify which are our independent and our target variables. In this case, \"text\" is our independent variable, because it is the one that can change. The target variable is the one we will ultimately test for. That's why we have an X train (text) and a X test (priority) and a Y train (text) and a Y test (priority). We designate that we want the test_size to be 20% (in decimals) and then we set a random state, which is simply something the computer needs to begin a random selection process. Changing the random state should not change the outcome of the model if the model is accurate; however, if you were to change the random_state variable, you are likely to come up with a different composition of testing and training datasets. Finally, we use `stratify` to make sure that we're selecting proportionately among the various priority labels. At this point, we just want to know if data is a priority or not. We don't want to bias the model by skewing it toward one priority type. (Note: we'll come back to this later). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5rv3l5ozsRt"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
        "                                                    df['Priority'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=df['Priority'])\n",
        "\n",
        "print('Size of Training Data ', X_train.shape[0])\n",
        "print('Size of Test Data ', X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct1WmrlXzsRt"
      },
      "source": [
        "## Step 3 - Training the machine learning model\n",
        "\n",
        "The next step should be familiar. We need to vectorize the text using the tf-idf vectorizer (which means that we want to make sure that words that appear very frequently and not frequently at all do not skew the model). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1VjwXaTzsRt"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
        "X_train_tf = tfidf.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx8KGDhP-zgR"
      },
      "source": [
        "Then we're going to use \"support vectors\" to train a model. Linear SVM is a very popular text classification algorithm. In this model, the computers uses the relationships between vectors to create a line where there is the greatest separation between two classes. There may be more than one possible line. The algorithm chooses the line that provides the maximum separation between the closest points in two classes. Because the computer needs to know where to start its random process of evaluating vectors, we have to set a `random_state`. The tolerance level is how much room for error we are willing to live with. In this case, it is input in scientific notation, and a much simpler way of indicating 0.00001. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgppqpz_zsRu"
      },
      "outputs": [],
      "source": [
        "# The output of the LinearSVC (Lindear Support Vector Classification) is a \n",
        "# model file, which is an object with multiple components. The model itself\n",
        "# isn't instructive or useful until we evaluate it and apply it to tasks.\n",
        "model1 = LinearSVC(random_state=0, tol=1e-5)\n",
        "model1.fit(X_train_tf, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI-DQR0gzsRu"
      },
      "source": [
        "## Step 4 - Model Evaluation\n",
        "We're going to spend more time thinking about evaluation during the last few weeks of class. It's important to go through these steps and to know what they do. It's impossible to completely separate any of these steps in the process. However, just note that we'll come back to this concept in coming weeks and spend more time on what it means to evaluate and analyze. Suffice it to say, analysis and evaluation are required at every and all stages of the process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCKvYEhUzsRu"
      },
      "outputs": [],
      "source": [
        "# Now we want to take the test dataset and we're going to use it\n",
        "# to evaluate how accurately the model can predict whether text is\n",
        "# associated with a priority or not. Notice we are skipping the fit\n",
        "# step and going straight to vectorizing. In other words, we are not \n",
        "# asking the model to learn new vocabulary. \n",
        "X_test_tf = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzCKh7tMzsRu"
      },
      "outputs": [],
      "source": [
        "# We evaluate how well the model did by running the predict method on the X test \n",
        "# sample of data and then compare the predicted values to the actual labels. \n",
        "Y_pred = model1.predict(X_test_tf)\n",
        "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtmGc2dcLHMZ"
      },
      "source": [
        "### Results\n",
        "The accuracy score is a proportion and can be interpreted as a percentage. In other words, there's an 87.6% (give or take) likelihood that the model will be abe to predict whether or not the text is associated with a Priority label. It's a good idea to consider whether or not the accuracy is more or less likely than other methods of estimating. For example, is it better than the odds of tossing a coin and getting it right? (better than 25%) Or, could you roll a dice and get a better likelihood of the right answer? \n",
        "\n",
        "So, one way to do this is to compare the result to something more naive like the DummyClassifier, where the baseline model always predicts the class with the highest frequency or that is \"stratified\" which is to say it makes predictions based on the proportion of labels in the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-Gz2egszsRu"
      },
      "outputs": [],
      "source": [
        "clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_pred_baseline = clf.predict(X_test)\n",
        "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adn-JloqMg76"
      },
      "source": [
        "Uh oh. Hold on a second... what we discover when we do this is that the trained model isn't actually predicting based on what we imagined to be other features. In other words, the model is accurate as often as if we assigned all the text with the P3 class... We might also want to go in and imagine how well the model is performing depending on the various priority levels. \n",
        "\n",
        "To do that, we can build something called a *confusion matrix*. In a [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62), we compare the actual values with the predicted values. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdCb0epqNpFV"
      },
      "source": [
        "\n",
        "In this case, the actual values are Priority P3 and Not P3 (P1+P2+P4+P5). The Predicted values are also Priority P3 and Not P3 (P1+P2+P4+P5). \n",
        "\n",
        "* True positive: the actual label and the predicted label P3 are both correct. \n",
        "* False positive: assigning text to any label that is not P3 is correct. \n",
        "* False negative: The number of times the text was predicted to be not P3 but in reality it was P3.  \n",
        "* True negative: The number of times the model precicts correctly that the text is not P3. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj2J_cYRzsRu"
      },
      "source": [
        "### Precision and Recall\n",
        "What makes a confusion matrix useful is that we can use this matrix of numbers to measure the proportion of predicted positives that were actually positive, which is another measure of how accurate the predictions are. \n",
        "\n",
        "Precision is the number of times the model correctly predicts a variable to be in a category, divided by the total number of accurate predictions. \n",
        "\n",
        "Recall is the number of times that the model makes the right class assignment (true positive) divided by the number of times it is completely correct and completely incorrect (True positive + false negative). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN1OVs1DzsRv"
      },
      "outputs": [],
      "source": [
        "Y_pred = model1.predict(X_test_tf)\n",
        "confusion_matrix(Y_test, Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9amY1qFRdk2"
      },
      "source": [
        "The problem is that we're not just testing for one label. We've got a multi-class question. There's not just one priority label. There are 5, and they are not equally represented in the data. So, while the model correctly predicts that there is a high priority for P3, it does not differentiate between classes of priority. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5_amFTwzsRv"
      },
      "outputs": [],
      "source": [
        "## Old code:\n",
        "\n",
        "# plot_confusion_matrix(model1,X_test_tf,\n",
        "#                       Y_test, values_format='d',\n",
        "#                       cmap=plt.cm.Blues)\n",
        "# plt.show()\n",
        "\n",
        "## New code:\n",
        "\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = model1.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h3Y0SYZzsRv"
      },
      "outputs": [],
      "source": [
        "print(classification_report(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8HyL1lJzsRv"
      },
      "source": [
        "### Class Imbalance\n",
        "Ultimately, the challenge is that the dataset has a much higher proportion of data labeled P3 than any other label. This is going to skew the predictions, because the most likely label is often the most frequent label. But frequency is only one feature out of many. \n",
        "\n",
        "Feature engineering includes figuring out how to adjust the input into the model to better reflect what you want it to focus on. So, for example, if we want it to consider that P3 is common, but not completely determine the model based on frequency, then we could choose to either adjust down the amount of P3 labeled data that we include in the model--called *downsampling*, or inversely, we could increase the amount of data with other labels (*upsampling). \n",
        "\n",
        "For our example, we're going to bring the amount of P3 data down so that it remains the largest category, but it is not purely size that will determine the model's output. When we downsample, we are losing information. If your dataset is already small, this may not be the best choice. If you are upsampling, then you are adding or repeating existing data. That will also impact data quality and lead to compromise. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubHg4vUozsRv"
      },
      "outputs": [],
      "source": [
        "# Filter bug reports with priority P3 and sample 4000 rows from it\n",
        "df_sampleP3 = df[df['Priority'] == 'P3'].sample(n=4000, random_state=123)\n",
        "\n",
        "# Create a separate dataframe containing all other bug reports\n",
        "df_sampleRest = df[df['Priority'] != 'P3']\n",
        "\n",
        "# Concatenate the two dataframes to create the new balanced bug reports dataset\n",
        "df_balanced = pd.concat([df_sampleRest, df_sampleP3])\n",
        "\n",
        "# Check the status of the class imbalance\n",
        "df_balanced['Priority'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL-hPHlJzsRv"
      },
      "source": [
        "# Final Blueprint for Text Classification\n",
        "\n",
        "Ok, so now that we've done each of these steps one time, let's go back and start the process over making adjustments to our model to reflect what we've learned about the data. We're going to turn this into a multiclass classification model. We've downsampled data labeled P3 so that frequency is not overdetermining the result. \n",
        "\n",
        "Despite the fact that the first model had a very high \"accuracy\" score, what we realized is that it wasn't actually a good model because it wasn't doing anything more than identifying all data as likely to be associated with the most frequent label. \n",
        "\n",
        "Now, we're going to try to create a new model that can predict whether an issue is a priority based on which of the labels it is most likely to be associated with. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-w6QIeAzsRw"
      },
      "outputs": [],
      "source": [
        "# Loading the balanced dataframe\n",
        "\n",
        "df = df_balanced[['text', 'Priority']]\n",
        "df = df.dropna()\n",
        "\n",
        "# Step 1 - Data Preparation\n",
        "\n",
        "df['text'] = df['text'].apply(clean)\n",
        "\n",
        "# Step 2 - Train-Test Split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
        "                                                    df['Priority'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=df['Priority'])\n",
        "print('Size of Training Data ', X_train.shape[0])\n",
        "print('Size of Test Data ', X_test.shape[0])\n",
        "\n",
        "# Step 3 - Training the Machine Learning model\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words=\"english\")\n",
        "X_train_tf = tfidf.fit_transform(X_train)\n",
        "\n",
        "model1 = LinearSVC(random_state=0, tol=1e-5)\n",
        "model1.fit(X_train_tf, Y_train)\n",
        "\n",
        "# Step 4 - Model Evaluation\n",
        "\n",
        "X_test_tf = tfidf.transform(X_test)\n",
        "Y_pred = model1.predict(X_test_tf)\n",
        "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25r0IGwYUhzD"
      },
      "source": [
        "So the new accuracy score for the model overall now is about 50% (it won't be the same for everyone because it's based on probabilities, but the difference won't be more than 1% up or down). The precision and recall values for P1 and P2 are better, but it completely misses the 50 bugs that had been labeled P5. \n",
        "\n",
        "Nevertheless, even though the model's accuracy score is lower, it is a better model overall because the predictions are more reflective of the input. And, what we learn, is that this isn't really going to be helpful for generating predictions in the future. In other words, if we run the DummyClassifier on the stratified data, we'll discover that simply making predictions about what priority label a bug report receives could be randomly assigned based on the overal proportions of each label in the dataset... and it would be just as accurate. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7_FMiKJzsRw"
      },
      "outputs": [],
      "source": [
        "clf = DummyClassifier(strategy='stratified', random_state=21)\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_pred_baseline = clf.predict(X_test)\n",
        "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh-uE95ZVs5R"
      },
      "source": [
        "Perhaps the whole exercise isn't a total loss, though, because we can go through and see places where the model was accurate and inaccurate and maybe there's something you can uncover about the quality of the data that is causing it to be problematic. Perhaps the results would change if you were to remove stopwords or symbols or numbers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ6TUYA2zsRw"
      },
      "outputs": [],
      "source": [
        "## Create a dataframe combining the Title and Description, \n",
        "## Actual and Predicted values that we can explore\n",
        "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred }\n",
        "result = pd.DataFrame(frame)\n",
        "\n",
        "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
        "       (result['actual'] == result['predicted'])].sample(2, random_state=22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogWkgmJPzsRw"
      },
      "outputs": [],
      "source": [
        "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
        "       (result['actual'] != result['predicted'])].sample(2, random_state=33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVKYLyjczsRw"
      },
      "source": [
        "# Cross-Validation\n",
        "The accuracy of the model that we created is somewhere around 48.7%, which is just under a coin's toss likelihood of being correct. That's not great. There are a number of steps we could take to try to improve the model from here: \n",
        "- add features like trigrams\n",
        "- add additional text cleaning steps\n",
        "- tweak the model's parameters and then check for performance on the test split. \n",
        "\n",
        "One way to test whether or not the results are an artifact of the random selection of data and the shuffling of data order could be to set a new `random_state` which would shuffle the data and could result in a different accuracy outcome. We won't know until we try. \n",
        "\n",
        "Cross-validation is when we train a model on different splits of the data and validate on different splits of data so that the model achieves a balance between *underfitting* and *overfitting*. Underfitting happens when the  model is complex and has fit the underlying pattern well during training but produces significant deviations during the testing phase. In this case, the model has been trained too narrowly on the data and doesn't do a good job of generalizing on new data. \n",
        "\n",
        "We can build cross-validation into the classification pipeline using something called K-fold validation. This is a formula, essentially, for selecting different cross sections of data and then returning validation scores for each \"fold\" or iteration of the split. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z28i_vH-zsRw"
      },
      "outputs": [],
      "source": [
        "# Vectorization\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
        "df_tf = tfidf.fit_transform(df['text']).toarray()\n",
        "\n",
        "# Cross Validation with 5 folds\n",
        "\n",
        "scores = cross_val_score(estimator=model1,\n",
        "                         X=df_tf,\n",
        "                         y=df['Priority'],\n",
        "                         cv=5)\n",
        "\n",
        "print (\"Validation scores from each iteration of the cross validation \", scores)\n",
        "print (\"Mean value across of validation scores \", scores.mean())\n",
        "print (\"Standard deviation of validation scores \", scores.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqgW6AjkzsRx"
      },
      "source": [
        "# Hyperparameter Tuning with Grid Search\n",
        "Grid-search is a tool that we can use to improve a model's accuracy by systematically evaluating the parameters of the arguments we feed to the model. The process includes trying out different combinations of *hyperparameters* to improve the desired output. [Hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning) set the boundaries of the \"learning\" process. When we use grid-search, we provide a set of parameter values that we can evaluate. Combining this with cross-validation will help to identify the hyperparameters that are impacting the model and provide clues as to where to make adjustments to improve the accuracy of the results. \n",
        "\n",
        "To begin, a `training_pipeline` defines the steps that we would like to run. The following example specifies tf-idf vectorization and the LinearSVC model from scikit-learn. Then, the `grid_param` argument allows us to define the parameters that we want to test. Parameters are specific to each phase in the pipeline, so we use the name of the phase as a prefix to specify what we're testing for. So, for example, min_df is a parameter, so we call it `tfidf_min_df`. In the last phase, we call the GridSearchCV method, which automates the process of testing multiple versions of the entire piepline with different sets of hyperparameters and produces a cross-validation score for each. That allows us to pick the best performing pipeline and model. \n",
        "\n",
        "The following step will take a bit of time. Be sure that you can let the process run without the notebook disconnecting so that it can finish. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ3tJ7EJzsRx"
      },
      "outputs": [],
      "source": [
        "training_pipeline = Pipeline(\n",
        "    steps=[('tfidf', TfidfVectorizer(\n",
        "        stop_words=\"english\")), ('model',\n",
        "                                 LinearSVC(random_state=21, tol=1e-5))])\n",
        "\n",
        "grid_param = [{\n",
        "    'tfidf__min_df': [5, 10],\n",
        "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
        "    'model__penalty': ['l2'],\n",
        "    'model__loss': ['hinge'],\n",
        "    'model__max_iter': [10000]\n",
        "}, {\n",
        "    'tfidf__min_df': [5, 10],\n",
        "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
        "    'model__C': [1, 10],\n",
        "    'model__tol': [1e-2, 1e-3]\n",
        "}]\n",
        "\n",
        "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
        "                                   param_grid=grid_param,\n",
        "                                   cv=5)\n",
        "gridSearchProcessor.fit(df['text'], df['Priority'])\n",
        "\n",
        "best_params = gridSearchProcessor.best_params_\n",
        "print(\"Best alpha parameter identified by grid search \", best_params)\n",
        "\n",
        "best_result = gridSearchProcessor.best_score_\n",
        "print(\"Best result identified by grid search \", best_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4leXtb7zsRx"
      },
      "outputs": [],
      "source": [
        "gridsearch_results = pd.DataFrame(gridSearchProcessor.cv_results_)\n",
        "gridsearch_results[['rank_test_score', 'mean_test_score',\n",
        "                    'params']].sort_values(by=['rank_test_score'])[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFVbwVv9zsRx"
      },
      "source": [
        "# Reviewing what we've done and an extended pipeline with optional components\n",
        "\n",
        "The activity below is very resource intensive, and if you are running it on your local desktop computer or with the free Google Colab account it will take a prohibitively long time to do. As a result, my request is that you read through the code below as the bare minimum. Running the final cells for this is not required. There are activities below these next 4 cells, though, that you are expected to complete. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Using what we've learned so far, let's go through the whole process of doing a multiclass text classification analysis using supervised learning and support vector classification. This time, let's identify the priority of the bug so that we can assign it to the appropriate development team (ie. consider a real world task to apply). Ideally, we would be able to do this automatically by using a classification model to determine which component of the software the bug report is referring to. All the bug reports have a feature called \"Component\" and values in that feature include Core, UI, and Doc. \n",
        "\n",
        "To complete this new task, we need to make the following changes: \n",
        "- add a step to include a grid-search that identifies the best hyperparameters and limts the number of options tested to increase runtime. \n",
        "- add an option t ouse the sklearn.svm.SVC function in order to compare the performance of this model to the LinearSVC. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VNyY80NSzsRy",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Flag that determines the choice of SVC (True) and LinearSVC (False)\n",
        "runSVC = True\n",
        "\n",
        "# Loading the dataframe\n",
        "\n",
        "file = \"eclipse_jdt.csv\"\n",
        "file = f\"{BASE_DIR}/data/jdt-bugs-dataset/eclipse_jdt.csv.gz\" ### real location\n",
        "df = pd.read_csv(file)\n",
        "df = df[['Title', 'Description', 'Component']]\n",
        "df = df.dropna()\n",
        "df['text'] = df['Title'] + df['Description']\n",
        "df = df.drop(columns=['Title', 'Description'])\n",
        "\n",
        "# Step 1 - Data Preparation\n",
        "df['text'] = df['text'].apply(clean)\n",
        "df = df[df['text'].str.len() > 50]\n",
        "\n",
        "if (runSVC):\n",
        "    # Sample the data when running SVC to ensure reasonable run-times\n",
        "    df = df.groupby('Component', as_index=False).apply(pd.DataFrame.sample,\n",
        "                                                       random_state=42,\n",
        "                                                       frac=.2)\n",
        "\n",
        "# Step 2 - Train-Test Split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
        "                                                    df['Component'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=df['Component'])\n",
        "print('Size of Training Data ', X_train.shape[0])\n",
        "print('Size of Test Data ', X_test.shape[0])\n",
        "\n",
        "# Step 3 - Training the Machine Learning model\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "\n",
        "if (runSVC):\n",
        "    model = SVC(random_state=42, probability=True)\n",
        "    grid_param = [{\n",
        "        'tfidf__min_df': [5, 10],\n",
        "        'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
        "        'model__C': [1, 100],\n",
        "        'model__kernel': ['linear']\n",
        "    }]\n",
        "else:\n",
        "    model = LinearSVC(random_state=42, tol=1e-5)\n",
        "    grid_param = {\n",
        "        'tfidf__min_df': [5, 10],\n",
        "        'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
        "        'model__C': [1, 100],\n",
        "        'model__loss': ['hinge']\n",
        "    }\n",
        "\n",
        "training_pipeline = Pipeline(\n",
        "    steps=[('tfidf', TfidfVectorizer(stop_words=\"english\")), ('model', model)])\n",
        "\n",
        "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
        "                                   param_grid=grid_param,\n",
        "                                   cv=5)\n",
        "\n",
        "gridSearchProcessor.fit(X_train, Y_train)\n",
        "\n",
        "best_params = gridSearchProcessor.best_params_\n",
        "print(\"Best alpha parameter identified by grid search \", best_params)\n",
        "\n",
        "best_result = gridSearchProcessor.best_score_\n",
        "print(\"Best result identified by grid search \", best_result)\n",
        "\n",
        "best_model = gridSearchProcessor.best_estimator_\n",
        "\n",
        "# Step 4 - Model Evaluation\n",
        "\n",
        "Y_pred = best_model.predict(X_test)\n",
        "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZOFJWQ_CzsRy"
      },
      "outputs": [],
      "source": [
        "clf = DummyClassifier(strategy='most_frequent', random_state=21)\n",
        "clf.fit(X_train, Y_train)\n",
        "Y_pred_baseline = clf.predict(X_test)\n",
        "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5M1FQWDfzsRy"
      },
      "outputs": [],
      "source": [
        "## Create a dataframe combining the Title and Description, \n",
        "## Actual and Predicted values that we can explore\n",
        "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred } \n",
        "result = pd.DataFrame(frame)\n",
        "\n",
        "result[result['actual'] == result['predicted']].sample(2, random_state=21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5PGWvhiCzsRy"
      },
      "outputs": [],
      "source": [
        "result[result['actual'] != result['predicted']].sample(2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D4ahFhjzsRy"
      },
      "source": [
        "# Reviewing what we've done\n",
        "The activities in this notebook demonstrate a generalizeable approach to doing text classification analysis with SVC algorithms. We move from data preparation through data splitting into training and testing, and then we used cross-validation methods to evaluate the accuracy of the model and to validate the verious hyperparameters that would produce an optimal model. We also found that the accuracy score alone might not be the best measure of the \"best\" iteration of the model that we produce. We learned that understanding the data an making informed interpretations throughout the pipeline is a critical part of the process. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LpnEzpyuwEy"
      },
      "source": [
        "# Reflection Activity\n",
        "\n",
        "Now that you've gone through the process of developing a supervised model for classifying text, are there parts of the process that you feel could be concerning for feminist scholarship? What are the opportunities? \n",
        "\n",
        "Thinking through the steps of the process, what kinds of questions should feminist scholars who are interested in doing classification analysis consider as they prepare data for analysis? As they evaluate the accuracy of the model? As they consider testing the model? \n",
        "\n",
        "Finally, what might be some of the opportunities or concerns be for feminist scholars who are interested in using a classification model workflow like this to make predictions about new data? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zDqfxTS2Dw-"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}